[
  {
    "objectID": "exploratory.html",
    "href": "exploratory.html",
    "title": "Analyzing Interactivity",
    "section": "",
    "text": "Show the code\n# packages\nlibrary(tidyverse)\nlibrary(bggUtils)\nlibrary(ggforce)\nlibrary(gt)\nlibrary(gtExtras)\n\n# functions\ntargets::tar_source(\"R\")\n\n# data\n# survey responses\nresponses = targets::tar_read(\"responses\")\ninteraction_table = targets::tar_read(\"interaction_table\")\n\n# games\ngames_raw = targets::tar_read(\"games_raw\")\ngames_preprocessed = targets::tar_read(\"games_preprocessed\")\n\n# ggplot theme\ntheme_set(bggUtils::theme_bgg())\n\n\n\n\nThe responses data has one record for each game from a respondent, with a rating ranging from 1 to 5. I can aggregate this to create a table with the number of votes for each rating, along with its mean and standard deviation.\n\n\nShow the code\nresponses |&gt;\n  format_as_table() |&gt;\n  arrange(desc(n)) |&gt;\n  select(name, starts_with(\"rating_\"), everything()) |&gt;\n  gt_ratings_tbl()\n\n\n\n\n\n\n\n\n\nTo see these games at a glance, I’ll plot the proportion of votes for each rating for all games, ranking them from most interactive to least interactive.\n\n\nShow the code\nresponses |&gt;\n  format_as_table() |&gt;\n  plot_responses_count()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nresponses |&gt;\n  group_by(name) |&gt;\n  summarize_responses() |&gt;\n  plot_mean_vs_sd()\n\n\n\n\n\n\n\n\n\n\n\n\nThe responses document is slightly different than data shown in the Interaction Table sheet, which has more responses from, I’m guessing, previous surveys? Nonetheless this can be analyzed in a similar fashion.\n\n\nShow the code\ninteraction_table |&gt;\n  gt_ratings_tbl()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ninteraction_table |&gt;\n  plot_responses_count()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ninteraction_table |&gt;\n  plot_mean_vs_sd()\n\n\n\n\n\n\n\n\n\nThis table has more data to work with, so I’m going to proceed using this rather than the raw responses, but this is something that can be easily flipped."
  },
  {
    "objectID": "exploratory.html#responses",
    "href": "exploratory.html#responses",
    "title": "Analyzing Interactivity",
    "section": "",
    "text": "The responses data has one record for each game from a respondent, with a rating ranging from 1 to 5. I can aggregate this to create a table with the number of votes for each rating, along with its mean and standard deviation.\n\n\nShow the code\nresponses |&gt;\n  format_as_table() |&gt;\n  arrange(desc(n)) |&gt;\n  select(name, starts_with(\"rating_\"), everything()) |&gt;\n  gt_ratings_tbl()\n\n\n\n\n\n\n\n\n\nTo see these games at a glance, I’ll plot the proportion of votes for each rating for all games, ranking them from most interactive to least interactive.\n\n\nShow the code\nresponses |&gt;\n  format_as_table() |&gt;\n  plot_responses_count()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nresponses |&gt;\n  group_by(name) |&gt;\n  summarize_responses() |&gt;\n  plot_mean_vs_sd()"
  },
  {
    "objectID": "exploratory.html#interaction-table",
    "href": "exploratory.html#interaction-table",
    "title": "Analyzing Interactivity",
    "section": "",
    "text": "The responses document is slightly different than data shown in the Interaction Table sheet, which has more responses from, I’m guessing, previous surveys? Nonetheless this can be analyzed in a similar fashion.\n\n\nShow the code\ninteraction_table |&gt;\n  gt_ratings_tbl()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ninteraction_table |&gt;\n  plot_responses_count()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ninteraction_table |&gt;\n  plot_mean_vs_sd()\n\n\n\n\n\n\n\n\n\nThis table has more data to work with, so I’m going to proceed using this rather than the raw responses, but this is something that can be easily flipped."
  },
  {
    "objectID": "exploratory.html#methodology",
    "href": "exploratory.html#methodology",
    "title": "Analyzing Interactivity",
    "section": "2.1 Methodology",
    "text": "2.1 Methodology\nHow can we get an interactivity rating for games other than the ones shown in blue? The simplest answer is to just collect a bunch of data - get the community to respond to surveys and rate all of the games!\nThat’s going to be difficult to accomplish, so my approach is to estimate the interactivity for all of these other games using the ratings we have in our sample. To do this, I will attempt to train a model to learn the relationship between attributues of games (as taken from BGG) and the interaction rating. If a model can learn how to predict this rating, we can then use the resulting model to estimate the interactive rating for games the community has not yet rated. These estimates can then be used to guide our labeling strategy for new games, which can then be fed back into the model, and so on.\nThis requires first assessing our ability to predict the interactive rating using information from BGG.\n\nTrain a model to predict interactivity as a function of BGG features.\nAssess model performance via resampling and hold-out set.\nUse model to predict games that do not have interactivity rating.\nUse model predictions to guide labeling strategy for games without interactivity rating.\nGo back to step 1 with expanded/updated interactivity rating\nRepeat 1-4\n\n\n\nShow the code\ntargets::tar_load(interaction_estimates)\n\ngames_and_interaction = \n  games_preprocessed |&gt;\n  group_by(name) |&gt;\n  slice_max(usersrated, n =1) |&gt;\n  ungroup() |&gt;\n  inner_join(\n    interaction_estimates\n  )\n\n\nWe are in a very small data setting, which means we need to be careful in how we spend our data. Ultimately, the goal is to develop a model to predict the interactivity of games that do not have a rating, where we can trust that our model predicts this rating reasonably well. This means we need to assess how well a model can perform on unseen data.\nTo this end, I will create a train/validation split for games with interaction ratings. I will use the training set to develop a model and the validation set to assess its performance in predicting games not used in training the model.\n\n\nShow the code\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(rstanarm)\nlibrary(mixOmics)\nlibrary(tailor)\n\ntidyverse::tidyverse_conflicts()\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ foreach::accumulate()   masks purrr::accumulate()\n✖ scales::col_factor()    masks readr::col_factor()\n✖ scales::discard()       masks purrr::discard()\n✖ Matrix::expand()        masks tidyr::expand()\n✖ dplyr::filter()         masks stats::filter()\n✖ recipes::fixed()        masks stringr::fixed()\n✖ jsonlite::flatten()     masks purrr::flatten()\n✖ rvest::guess_encoding() masks readr::guess_encoding()\n✖ dplyr::lag()            masks stats::lag()\n✖ mixOmics::map()         masks purrr::map()\n✖ Matrix::pack()          masks tidyr::pack()\n✖ MASS::select()          masks gtExtras::select(), dplyr::select()\n✖ yardstick::spec()       masks readr::spec()\n✖ Matrix::unpack()        masks tidyr::unpack()\n✖ foreach::when()         masks purrr::when()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow the code\ntidymodels::tidymodels_prefer()\n\n\nset.seed(1999)\nsplit = \n  games_and_interaction |&gt;\n  initial_split(strata = interaction)\n\nset.seed(1999)\nfolds = \n  split |&gt;\n  training() |&gt;\n  vfold_cv(v = 5, strata = interaction, repeats = 3)"
  },
  {
    "objectID": "exploratory.html#preprocessing",
    "href": "exploratory.html#preprocessing",
    "title": "Analyzing Interactivity",
    "section": "2.2 Preprocessing",
    "text": "2.2 Preprocessing\nWhat might predict a game’s interactive rating? I have a very rich set features for games, including everything from player counts, playing time, publishers, mechanics, categories, components, and so on. It would be tempting to take the kitchen sink approach and throw everything into a model and see what we get, but given that we have such a small dataset I would prefer to start with small set of features that we think would explain interactivity and add from there.\nMy expectation is that player counts and playing time will play a role, along with certain types of mechanics and categories. I’ll create a couple of different recipes, or preprocessors, that will generate different sets of features.\n\n\nShow the code\n# recipe with very minimal set of features features\nbaseline_recipe = \n  split |&gt;\n  training() |&gt;\n  build_recipe(outcome = interaction,\n               predictors = c(\"minplayers\", \n                              \"maxplayers\",\n                              \"playingtime\",\n                              \"maxplaytime\",\n                              \"minplaytime\",\n                              \"minage\")\n  ) |&gt;\n  step_playingtime_and_playercount() |&gt;\n  # step_playercounts(threshold = 50) |&gt;\n  add_zv()\n\n# expanded features; adding in mechanics and categoreis\nexpanded_recipe = \n  split |&gt;\n  training() |&gt;\n  build_recipe(outcome = interaction,\n               predictors = c(\"minplayers\", \n                              \"maxplayers\",\n                              \"playingtime\",\n                              \"maxplaytime\",\n                              \"minplaytime\",\n                              \"minage\",\n                              \"mechanics\",\n                              \"categories\")\n  ) |&gt;\n  step_playingtime_and_playercount() |&gt;\n  step_count_items(mechanics, categories) |&gt;\n  add_dummies(mechanics, threshold = 15) |&gt;\n  add_dummies(categories, threshold = 15)\n\n# normalize\nnorm_recipe = \n  expanded_recipe |&gt;\n  add_zv() |&gt;\n  add_normalize()\n\n\n# trying out playercount dummies? idk\nplayercounts_recipe = \n  split |&gt;\n  training() |&gt;\n  build_recipe(outcome = interaction,\n               predictors = c(\"minplayers\", \n                              \"maxplayers\",\n                              \"playingtime\",\n                              \"maxplaytime\",\n                              \"minplaytime\",\n                              \"minage\",\n                              \"mechanics\",\n                              \"categories\")\n  ) |&gt;\n  step_playingtime_and_playercount() |&gt;\n  step_dummy_playercounts(threshold = 50) |&gt;\n  step_count_items(mechanics, categories) |&gt;\n  add_dummies(mechanics, threshold = 15) |&gt;\n  add_dummies(categories, threshold = 15) |&gt;\n  add_zv() |&gt;\n  add_normalize()\n\n# pca\npca_recipe = \n  norm_recipe |&gt;\n  step_pca(all_numeric_predictors(), id = \"pca\", threshold = .50)\n\n# pls\npls_recipe = \n  norm_recipe |&gt;\n  step_pls(all_numeric_predictors(), outcome = \"interaction\", id = \"pls\", num_comp = 5)\n\n\nJust to illustrate, I’ll take one of these recipes and inspect how interaction varies along with some of the features we have. Looking first at numeric features via a scatterplot.\n\n\nShow the code\nbaked = \n  expanded_recipe |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nbaked |&gt;\n  select(-starts_with(\"mechanics_\"), -starts_with(\"categories\")) |&gt;\n  pivot_longer(\n    cols = -c(game_id, name, yearpublished, interaction),\n    names_to = c(\"feature\"),\n    values_to = c(\"value\")\n  ) |&gt;\n  ggplot(aes(x=interaction, y=value))+\n  geom_point(size = 0.5)+\n  facet_wrap(feature ~., scales = \"free\")+\n  geom_smooth(method = 'loess', formula = 'y~x')\n\n\n\n\n\n\n\n\n\nNext looking at mechanics. As we would expect, there are certain mechanics that are a dead giveaway that a game will be more interactive, such as Take-That and Area Majority, while other mechanics indicate a game is more of a Euro and less interactive, such as Worker/Tile Placement. This makes me wonder whether a model trained to predict interactivity is really just going to be picking up on the differences between “American” (thematic, conflict) and “European” (economy, efficiency) schools of design?\n\n\nShow the code\npivot_categorical = function(data) {\n  \n  data |&gt;\n    pivot_longer(\n      cols = -c(game_id, name, yearpublished, interaction),\n      names_to = c(\"feature\"),\n      values_to = c(\"value\")\n    ) |&gt;\n    mutate(value = case_when(value == 1 ~ 'yes', TRUE ~ 'no')) \n  \n}\n\nplot_categorical = function(data, ncol = 5) {\n  \n  data |&gt;\n    ggplot(aes(x=interaction, y=value, color = value))+\n    # geom_boxplot()+\n    geom_point(size = 0.5, alpha = 0.25, position = ggforce::position_jitternormal(sd_x = 0))+\n    geom_boxplot(alpha = 0.5, outliers = F)+\n    facet_wrap(~bggUtils::present_bgg_text(feature), scales = \"free\", ncol = ncol)+\n    theme(strip.text.x = element_text(size = 6))+\n    guides(color = 'none')+\n    ylab(\"feature\")+\n    scale_color_viridis_d(begin = 0.8, end = 0) +\n    scale_fill_viridis_d(begin = 0.8, end = 0)\n}\n\nboxplot_categorical = function(data) {\n  \n  data |&gt;\n    filter(value == 'yes') |&gt;\n    mutate(feature = bggUtils::present_bgg_text(feature)) |&gt;\n    group_by(feature) |&gt;\n    mutate(mean_rating = mean(interaction)) |&gt;\n    ungroup() |&gt;\n    ggplot(aes(x=interaction, y=reorder(feature, mean_rating)))+\n    geom_boxplot(alpha = 0.5, outliers = F)+\n    geom_point(size = 0.1, alpha = 0.5, position = ggforce::position_jitternormal(sd_x = 0))+\n    ylab(\"\")+\n    xlab(\"interaction rating\")\n  \n}\n\ndotplot_categorical = function(data) {\n  \n  data |&gt;\n    group_by(feature, value) |&gt; \n    summarize(mean = mean(interaction)) |&gt; \n    mutate(feature = bggUtils::present_bgg_text(feature)) |&gt;\n    ggplot(aes(x=mean, y=reorder(feature, mean)))+\n    geom_line(aes(group = feature), color = 'grey60')+\n    geom_point(aes(color = value))+\n    ylab(\"feature\")+\n    xlab(\"mean interaction rating\")\n  \n}\n\n\nbaked |&gt;\n  select(game_id, name, yearpublished, interaction, starts_with(\"mechanics_\")) |&gt;\n  pivot_categorical() |&gt;\n  mutate(feature = gsub(\"^mechanics_\", \"\", feature)) |&gt;\n  boxplot_categorical()+\n  theme_light()+\n  labs(title = \"Interactivity by Mechanic\")\n\n\n\n\n\n\n\n\n\nWe notice something similar if we look at categories, with Fantasy and Fighting games tending to be more interactive than Economic games. Here we can also notice that certain categories that we would associate with party/social deduction games (Bluffing, Deduction) tend to have higher player interaction.\n\n\nShow the code\nbaked |&gt;\n  select(game_id, name, yearpublished, interaction, starts_with(\"categories_\")) |&gt;\n  pivot_categorical() |&gt;\n  mutate(feature = gsub(\"^categories_\", \"\", feature)) |&gt;\n  boxplot_categorical()+\n  theme_light()+\n  labs(title = \"Interactivity by Category\")\n\n\n\n\n\n\n\n\n\nI created a specific recipe just to deal with playercounts - I’m thinking that the best way to handle player counts might be to simply create dummies for individual player counts rather than just use the minimum and maximum player count.\n\n\nShow the code\nplayercounts_recipe |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  select(game_id, name, interaction, starts_with(\"player_counts\")) |&gt;\n  pivot_longer(cols = starts_with(\"player_counts\"),\n               names_to = c(\"feature\")) |&gt;\n  mutate(value = case_when(value &gt; 0 ~ 'yes',\n                           value &lt; 0 ~ 'no')) |&gt;\n  plot_categorical(ncol = 2)"
  },
  {
    "objectID": "exploratory.html#models",
    "href": "exploratory.html#models",
    "title": "Analyzing Interactivity",
    "section": "2.3 Models",
    "text": "2.3 Models\nNow create model specifications. I’ll use models in the linear family for now, given the amount of data we have to train a model on. I’m just using a simple linear model, as well as a linear model with a penalty (glmnet).\n\n\nShow the code\nlm_mod = linear_reg()\nglmnet_mod = linear_reg(engine = \"glmnet\", \n                        penalty = tune::tune())\n\n\nI’ll then combine these models with the recipes to create a set of candidate workflows, which I will then tune and evaluate across resampling. I’m evaluating models using standard measures (rmse, rsquared, mape), but I’m also examining the concordance correlation coefficient (ccc), as I think we really want to balance both accuracy and consistency.\n\n\nShow the code\nwflows = \n  workflowsets::workflow_set(preproc = list(\"baseline\"  = baseline_recipe,\n                                            \"expanded\" = expanded_recipe,\n                                            \"playercounts\" = playercounts_recipe,\n                                            \"pca\" = pca_recipe,\n                                            \"pls\" = pls_recipe),\n                             models = list(\"lm\" = lm_mod,\n                                           #  \"stan\" = stan_mod,\n                                           \"glmnet\" = glmnet_mod)\n  ) |&gt;\n  anti_join(\n    tibble(wflow_id = c(\"pls_glmnet\", \"pca_glmnet\")),\n    by = \"wflow_id\"\n  )"
  },
  {
    "objectID": "exploratory.html#results",
    "href": "exploratory.html#results",
    "title": "Analyzing Interactivity",
    "section": "2.4 Results",
    "text": "2.4 Results\nI’ll now examine how each of these workflows performed during resampling.\n\n\nShow the code\nset.seed(1999)\nmodels = \n  wflows |&gt;\n  workflow_map(fn = \"tune_grid\",\n               resamples = folds,\n               verbose = T,\n               grid = 15,\n               metrics = metric_set(rmse, rsq, mape, ccc),\n               control = control_grid(save_pred = T, save_workflow = T)\n  )\n\n\nHow did they all do? I’ll just grab the resulting performance for every candidate model and plot it for each metric.\n\n\nShow the code\nmodels |&gt;\n  autoplot(type = \"wflow_id\")+\n  theme_light()\n\n\n\n\n\n\n\n\n\nNotice that there are more workflows shown in the plot than the ones that I fit - this is because for each of the glmnet models we are further evaluating a set of candidate models over its tuning parameter.\nI’ll just extract the best performing model for each workflow and see how it performs across these metrics. Right now, the best performing model via resampling is the penalized model that includes a dummy for specific playercounts.\n\n\nShow the code\nmodels |&gt; \n  rank_results(select_best = T, rank_metric = \"ccc\") |&gt;\n  select(wflow_id, .metric, mean, std_err, n, rank) |&gt;\n  gt_tbl()|&gt;\n  gt::fmt_number(columns = c(\"mean\", \"std_err\"), decimals = 3)\n\n\n\n\n\n  \n    \n      wflow_id\n      .metric\n      mean\n      std_err\n      n\n      rank\n    \n  \n  \n    playercounts_glmnet\nccc\n0.681\n0.022\n15\n1\n    playercounts_glmnet\nmape\n17.739\n0.572\n15\n1\n    playercounts_glmnet\nrmse\n0.653\n0.017\n15\n1\n    playercounts_glmnet\nrsq\n0.509\n0.028\n15\n1\n    playercounts_lm\nccc\n0.679\n0.023\n15\n2\n    playercounts_lm\nmape\n17.956\n0.584\n15\n2\n    playercounts_lm\nrmse\n0.665\n0.018\n15\n2\n    playercounts_lm\nrsq\n0.498\n0.028\n15\n2\n    expanded_lm\nccc\n0.650\n0.028\n15\n3\n    expanded_lm\nmape\n18.693\n0.706\n15\n3\n    expanded_lm\nrmse\n0.689\n0.021\n15\n3\n    expanded_lm\nrsq\n0.463\n0.034\n15\n3\n    expanded_glmnet\nccc\n0.649\n0.028\n15\n4\n    expanded_glmnet\nmape\n18.693\n0.706\n15\n4\n    expanded_glmnet\nrmse\n0.688\n0.021\n15\n4\n    expanded_glmnet\nrsq\n0.463\n0.034\n15\n4\n    pls_lm\nccc\n0.618\n0.029\n15\n5\n    pls_lm\nmape\n19.512\n0.753\n15\n5\n    pls_lm\nrmse\n0.708\n0.023\n15\n5\n    pls_lm\nrsq\n0.427\n0.034\n15\n5\n    pca_lm\nccc\n0.572\n0.017\n15\n6\n    pca_lm\nmape\n18.938\n0.574\n15\n6\n    pca_lm\nrmse\n0.695\n0.010\n15\n6\n    pca_lm\nrsq\n0.423\n0.017\n15\n6\n    baseline_lm\nccc\n0.406\n0.029\n15\n7\n    baseline_lm\nmape\n22.205\n0.707\n15\n7\n    baseline_lm\nrmse\n0.790\n0.017\n15\n7\n    baseline_lm\nrsq\n0.259\n0.029\n15\n7\n    baseline_glmnet\nccc\n0.404\n0.029\n15\n8\n    baseline_glmnet\nmape\n22.218\n0.702\n15\n8\n    baseline_glmnet\nrmse\n0.790\n0.017\n15\n8\n    baseline_glmnet\nrsq\n0.259\n0.029\n15\n8\n  \n  \n  \n\n\n\n\nI’ll extract predictions from the models and compare the predictions to the actual.\n\n\nShow the code\nmodels |&gt;\n  collect_predictions(select_best = T, metric = 'ccc') |&gt;\n  left_join(\n    split |&gt;\n      training() |&gt;\n      mutate(.row = row_number()) |&gt;\n      select(game_id, name, .row)\n  ) |&gt;\n  select(wflow_id, game_id, name, .row, .pred, interaction) |&gt;\n  ggplot(aes(x=.pred, y=interaction))+\n  geom_point(size = 0.5)+\n  facet_wrap(~wflow_id)+\n  theme_light()+\n  ggpubr::stat_cor()+\n  geom_abline(slope = 1, linetype = 'dashed')\n\n\n\n\n\n\n\n\n\nThe best performing model at this stage is the penalized regression with playercount dummies. I’ll fit this model to the entirety of the training set to grab the coefficients and get a sense of what it’s learning from the data. I’ll also add a model postprocessor to constrain predictions between 1 and 5.\nLooking at the coefficients, we can see (as before) that certain mechanics (Take That, Area Majority), categories (Deduction, Territory Building), and player counts (5 players) are associated with higher interactivity ratings. Mechanics associated with Euro style games (End Game Bonuses, Tile Placement) and low player counts (x2, x3) tend to reduce interactivity.\n\n\nShow the code\n# fit \nbest_mod = \n  models |&gt;\n  fit_best(metric = \"ccc\")  |&gt;\n  add_tailor(\n    tailor() |&gt;\n      adjust_numeric_range(lower_limit = 1, upper_limit = 5)\n  )\n\n# extract coefs\nbest_mod |&gt;\n  tidy() |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  slice_max(abs(estimate), n = 35) |&gt;\n  mutate(sign = case_when(estimate &gt; 0 ~ 'increases interactivity',\n                          estimate &lt; 0 ~ 'decreases interactivity')) |&gt;\n  mutate(tidy_term = bggUtils::present_bgg_text(term)) |&gt;\n  ggplot(aes(y=reorder(tidy_term, estimate),\n             x=estimate))+\n  geom_col()+\n  ylab(\"\")+\n  facet_wrap(~sign, scales = \"free_x\")\n\n\n\n\n\n\n\n\n\nI’ll then use this model to predict the validation set. We’d like to see the our estimated interactive rating be pretty close to the actual here.\n\n\nShow the code\nvalid_preds = \n  best_mod |&gt;\n  augment(\n    split |&gt;\n      testing()\n  )\n\nplot_predictions = function(data) {\n  \n  data |&gt;\n    ggplot(aes(x=.pred, y=interaction, label = name))+\n    geom_point(size = 1.5)+\n    geom_text(size = 1.5, vjust = 1.5)+\n    coord_cartesian(xlim = c(0.5, 5.5), ylim = c(0.5, 5.5))+\n    geom_abline(slope = 1,\n                linetype = 'dashed')+\n    xlab(\"estimated interactivity\")+\n    ylab(\"actual interactivity\")+\n    ggpubr::stat_cor()\n  \n}\n\nvalid_preds |&gt; \n  ggplot(aes(x=.pred, y=interaction, label = name))+\n  geom_point(size = 1.5)+\n  geom_text(size = 1.5, vjust = 1.5)+\n  coord_cartesian(xlim = c(0.5, 5.5), ylim = c(0.5, 5.5))+\n  geom_abline(slope = 1,\n              linetype = 'dashed')+\n  xlab(\"estimated interactivity\")+\n  ylab(\"actual interactivity\")+\n  ggpubr::stat_cor()\n\n\n\n\n\n\n\n\n\nThe biggest misses for this version of the model look to be Rairoad Ink and Troyes. I’d guess that the model thinks Railroad Ink has more interaction than it does because it has a low play time and can play at higher playercounts?\nTroyes I can see a model failing to pick up on - on paper it’s a dry Euro game, but it’s actually quite interactive because of the manners in you can buy each other’s dice.\nOverall, the model’s performance on the validation set is in line with what we found during resampling.\n\n\nShow the code\nmetrics = metric_set(rmse, rsq, mape, rsq)\n\nbest_mod |&gt;\n  augment(\n    split |&gt;\n      testing()\n  ) |&gt;\n  metrics(\n    interaction,\n    .pred\n  ) |&gt;\n  mutate_if(is.numeric, round, 3) |&gt;\n  gt_tbl()\n\n\n\n\n\n  \n    \n      .metric\n      .estimator\n      .estimate\n    \n  \n  \n    rmse\nstandard\n0.582\n    rsq\nstandard\n0.586\n    mape\nstandard\n16.990\n    rsq\nstandard\n0.586\n  \n  \n  \n\n\n\n\nI’ll use resampling with this model across the entirety of the data we have for interactivity to get out of sample predictions for every game.\n\n\nShow the code\nset.seed(1999)\nfolds_all = split$data |&gt; vfold_cv(strata = interaction, repeats = 3)\n\noos_preds = \n  best_mod |&gt;\n  fit_resamples(resamples = folds_all,\n                control = control_resamples(save_pred = T))\n\noos_estimates = \n  oos_preds |&gt;\n  collect_predictions(summarize = T) |&gt;\n  select(.row, .pred) |&gt;\n  left_join(\n    split$data |&gt;\n      mutate(.row = row_number()) |&gt;\n      select(.row, game_id, name, yearpublished, interaction)\n  )\n\n\n\n\nShow the code\noos_estimates |&gt;\n  plot_predictions()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nestimates_tbl = function(data){\n  \n  data |&gt;\n    arrange(desc(interaction)) |&gt;\n    gt_tbl() |&gt;\n    gt::data_color(columns = c(\"estimate\", \"interaction\"),\n                   method = \"numeric\",\n                   palette = \"viridis\",\n                   domain = c(1, 5),\n                   reverse = T) |&gt;\n    gt::cols_align(columns = c(\"estimate\", \"interaction\"),\n                   align = \"center\") |&gt;\n    gt::opt_interactive()\n}\n\n\noos_estimates |&gt;\n  mutate_if(is.numeric, round, 3) |&gt;\n  select(game_id, name, estimate = .pred, interaction) |&gt;\n  arrange(desc(interaction)) |&gt;\n  estimates_tbl()"
  },
  {
    "objectID": "exploratory.html#the-top-100",
    "href": "exploratory.html#the-top-100",
    "title": "Analyzing Interactivity",
    "section": "3.1 The Top 100",
    "text": "3.1 The Top 100\nMaybe the quickest way to just get a sense of the interaction rating is to zoom in on a set of well-known games. Let’s look at the estimated interactivity rating for the BGG Top 100.\n\n\nShow the code\ntop_100 = \n  estimates |&gt;\n  rename(interaction = .pred) |&gt;\n  slice_max(bayesaverage, n = 100) |&gt;\n  select(game_id, name, geek_rating = bayesaverage, interaction) |&gt;\n  mutate_if(is.numeric, round, 3) |&gt;\n  arrange(interaction)\n\n\nWhich games in the top 100 are considered the least interactive? Everdell, Castles of Burgundy, On Mars, Ark Nova. We basically get a list of Euro style games that are a bit more on the multiplayer solitaire side.\n\n\nShow the code\ntop_100 |&gt;\n  gt_tbl() |&gt;\n  gt::opt_interactive() |&gt;\n  gt::data_color(columns = c(\"interaction\"),\n                 method = \"numeric\",\n                 palette = \"viridis\",\n                 domain = c(1, 5),\n                 reverse = T) |&gt;\n  gt::cols_align(columns = c(\"interaction\", \"geek_rating\"),\n                 align = \"center\") |&gt;\n  gt::opt_interactive()\n\n\n\n\n\n\n\n\n\nWhich games in the top 100 are considered the most interactive? TI4, Root, Eclipse, War of the Ring. Interactivity, in this case, is really picking up on the war/conflict side of things, though Five Tribes is one that’s a lot higher up there than I would have expected?\n\n\nShow the code\ntop_100 |&gt;\n  arrange(desc(interaction)) |&gt;\n  gt_tbl() |&gt;\n  gt::opt_interactive() |&gt;\n  gt::data_color(columns = c(\"interaction\"),\n                 method = \"numeric\",\n                 palette = \"viridis\",\n                 domain = c(1, 5),\n                 reverse = T) |&gt;\n  gt::cols_align(columns = c(\"interaction\", \"geek_rating\"),\n                 align = \"center\") |&gt;\n  gt::opt_interactive()"
  },
  {
    "objectID": "exploratory.html#top-1000",
    "href": "exploratory.html#top-1000",
    "title": "Analyzing Interactivity",
    "section": "3.2 Top 1000",
    "text": "3.2 Top 1000\nFinally, here’s the top 1000 games on BGG (just to make this table easier to) to sort through and examine along with other BGG ratings.\n\n\nShow the code\nestimates |&gt;\n  slice_max(bayesaverage, n = 1000) |&gt;\n  select(game_id, name, interaction = .pred, geek_rating = bayesaverage, averageweight, average) |&gt;\n  mutate_if(is.numeric, round, 3) |&gt;\n  gt_tbl() |&gt;\n  gt::opt_interactive() |&gt;\n  gt::data_color(columns = c(\"interaction\"),\n                 method = \"numeric\",\n                 palette = \"viridis\",\n                 domain = c(1, 5),\n                 reverse = T) |&gt;\n  gt::cols_align(columns = c(\"interaction\", \"geek_rating\", \"averageweight\"),\n                 align = \"center\") |&gt;\n  gt::opt_interactive()\n\n\n\n\n\n\n\n\n\nI’ll also write out these estimates and save them to a CSV.\n\n\nShow the code\nestimates |&gt;\n  select(game_id, name, interaction = .pred) |&gt;\n  write.csv(file = \"data/processed/estimates.csv\")"
  }
]