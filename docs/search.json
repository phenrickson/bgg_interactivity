[
  {
    "objectID": "exploratory.html",
    "href": "exploratory.html",
    "title": "Analyzing Interactivity",
    "section": "",
    "text": "Show the code\n# packages\nlibrary(tidyverse)\nlibrary(bggUtils)\nlibrary(ggforce)\nlibrary(gt)\nlibrary(gtExtras)\n\n# functions\ntargets::tar_source(\"R\")\n\n# data\n# survey responses\nresponses = targets::tar_read(\"responses\")\ninteraction_table = targets::tar_read(\"interaction_table\")\n\n# games\ngames_raw = targets::tar_read(\"games_raw\")\ngames_preprocessed = targets::tar_read(\"games_preprocessed\")\n\n# ggplot theme\ntheme_set(theme_bgg())\n\n\n\n\nThe responses data has one record for each game from a respondent, with a rating ranging from 1 to 5. I can aggregate this to create a table with the number of votes for each rating, along with its mean and standard deviation.\n\n\nShow the code\nresponses |&gt;\n  format_as_table() |&gt;\n  arrange(desc(n)) |&gt;\n  select(name, starts_with(\"rating_\"), everything()) |&gt;\n  gt_ratings_tbl()\n\n\n\n\n\n\n\n\n\nTo see these games at a glance, I’ll plot the proportion of votes for each rating for all games, ranking them from most interactive to least interactive.\n\n\nShow the code\nresponses |&gt;\n  format_as_table() |&gt;\n  plot_responses_count()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nresponses |&gt;\n  group_by(name) |&gt;\n  summarize_responses() |&gt;\n  plot_mean_vs_sd()\n\n\n\n\n\n\n\n\n\n\n\n\nThe responses document is slightly different than data shown in the Interaction Table sheet, which has more responses from, I’m guessing, previous surveys? Nonetheless this can be analyzed in a similar fashion.\n\n\nShow the code\ninteraction_table |&gt;\n  gt_ratings_tbl()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ninteraction_table |&gt;\n  plot_responses_count()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ninteraction_table |&gt;\n  plot_mean_vs_sd()\n\n\n\n\n\n\n\n\n\nThis table has more data to work with, so I’m going to proceed using this rather than the raw responses, but this is something that can be easily flipped."
  },
  {
    "objectID": "exploratory.html#responses",
    "href": "exploratory.html#responses",
    "title": "Analyzing Interactivity",
    "section": "",
    "text": "The responses data has one record for each game from a respondent, with a rating ranging from 1 to 5. I can aggregate this to create a table with the number of votes for each rating, along with its mean and standard deviation.\n\n\nShow the code\nresponses |&gt;\n  format_as_table() |&gt;\n  arrange(desc(n)) |&gt;\n  select(name, starts_with(\"rating_\"), everything()) |&gt;\n  gt_ratings_tbl()\n\n\n\n\n\n\n\n\n\nTo see these games at a glance, I’ll plot the proportion of votes for each rating for all games, ranking them from most interactive to least interactive.\n\n\nShow the code\nresponses |&gt;\n  format_as_table() |&gt;\n  plot_responses_count()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nresponses |&gt;\n  group_by(name) |&gt;\n  summarize_responses() |&gt;\n  plot_mean_vs_sd()"
  },
  {
    "objectID": "exploratory.html#interaction-table",
    "href": "exploratory.html#interaction-table",
    "title": "Analyzing Interactivity",
    "section": "",
    "text": "The responses document is slightly different than data shown in the Interaction Table sheet, which has more responses from, I’m guessing, previous surveys? Nonetheless this can be analyzed in a similar fashion.\n\n\nShow the code\ninteraction_table |&gt;\n  gt_ratings_tbl()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ninteraction_table |&gt;\n  plot_responses_count()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ninteraction_table |&gt;\n  plot_mean_vs_sd()\n\n\n\n\n\n\n\n\n\nThis table has more data to work with, so I’m going to proceed using this rather than the raw responses, but this is something that can be easily flipped."
  },
  {
    "objectID": "exploratory.html#methodology",
    "href": "exploratory.html#methodology",
    "title": "Analyzing Interactivity",
    "section": "2.1 Methodology",
    "text": "2.1 Methodology\nHow can we get an interactivity rating for games other than the ones shown in blue? The simplest answer is to just collect a bunch of data - get the community to respond to surveys and rate all of the games!\nThat’s going to be difficult to accomplish, so my approach is to estimate the interactivity for all of these other games using the ratings we have in our sample. To do this, I will attempt to train a model to learn the relationship between attributues of games (as taken from BGG) and the interaction rating. If a model can learn how to predict this rating, we can then use the resulting model to estimate the interactive rating for games the community has not yet rated. These estimates can then be used to guide our labeling strategy for new games, which can then be fed back into the model, and so on.\nThis requires first assessing our ability to predict the interactive rating using information from BGG.\n\nTrain a model to predict interactivity as a function of BGG features.\nAssess model performance via resampling and hold-out set.\nUse model to predict games that do not have interactivity rating.\nUse model predictions to guide labeling strategy for games without interactivity rating.\nGo back to step 1 with expanded/updated interactivity rating\nRepeat 1-4\n\n\n\nShow the code\ntargets::tar_load(interaction_estimates)\n\ngames_and_interaction = \n  games_preprocessed |&gt;\n  group_by(name) |&gt;\n  slice_max(usersrated, n =1) |&gt;\n  ungroup() |&gt;\n  inner_join(\n    interaction_estimates\n  )\n\n\nWe are in a very small data setting, which means we need to be careful in how we spend our data. Ultimately, the goal is to develop a model to predict the interactivity of games that do not have a rating, where we can trust that our model predicts this rating reasonably well. This means we need to assess how well a model can perform on unseen data.\nTo this end, I will create a train/validation split for games with interaction ratings. I will use the training set to develop a model and the validation set to assess its performance in predicting games not used in training the model.\n\n\nShow the code\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(mixOmics)\nlibrary(tailor)\nlibrary(treemap)\nlibrary(plsmod)\n\nsuppressMessages({\n  tidymodels::tidymodels_prefer()\n})\n\nset.seed(1999)\nsplit = \n  games_and_interaction |&gt;\n  initial_split(strata = interaction)\n\nset.seed(1999)\nfolds = \n  split |&gt;\n  training() |&gt;\n  vfold_cv(v = 5, strata = interaction, repeats = 3)"
  },
  {
    "objectID": "exploratory.html#preprocessing",
    "href": "exploratory.html#preprocessing",
    "title": "Analyzing Interactivity",
    "section": "2.2 Preprocessing",
    "text": "2.2 Preprocessing\nWhat might predict a game’s interactive rating? I have a very rich set features for games, including everything from player counts, playing time, publishers, mechanics, categories, components, and so on. It would be tempting to take the kitchen sink approach and throw everything into a model and see what we get, but given that we have such a small dataset I would prefer to start with small set of features that we think would explain interactivity and add from there.\nMy expectation is that player counts and playing time will play a role, along with certain types of mechanics and categories. I’ll create a couple of different recipes, or preprocessors, that will generate different sets of features.\n\n\nShow the code\n# recipe with very minimal set of features features\nbaseline_recipe = \n  split |&gt;\n  training() |&gt;\n  build_recipe(outcome = interaction,\n               predictors = c(\"minplayers\", \n                              \"maxplayers\",\n                              \"playingtime\",\n                              \"maxplaytime\",\n                              \"minplaytime\",\n                              \"minage\")\n  ) |&gt;\n  step_playingtime_and_playercount() |&gt;\n  # step_playercounts(threshold = 50) |&gt;\n  add_zv()\n\n# expanded features; adding in mechanics and categoreis\nexpanded_recipe = \n  split |&gt;\n  training() |&gt;\n  build_recipe(outcome = interaction,\n               predictors = c(\"minplayers\", \n                              \"maxplayers\",\n                              \"playingtime\",\n                              \"maxplaytime\",\n                              \"minplaytime\",\n                              \"minage\",\n                              \"mechanics\",\n                              \"categories\")\n  ) |&gt;\n  step_playingtime_and_playercount() |&gt;\n  step_count_items(mechanics, categories) |&gt;\n  add_dummies(mechanics, threshold = 15) |&gt;\n  add_dummies(categories, threshold = 15)\n\n# normalize\nnorm_recipe = \n  expanded_recipe |&gt;\n  add_zv() |&gt;\n  add_normalize()\n\n\n# trying out playercount dummies? idk\nplayercounts_recipe = \n  split |&gt;\n  training() |&gt;\n  build_recipe(outcome = interaction,\n               predictors = c(\"minplayers\", \n                              \"maxplayers\",\n                              \"playingtime\",\n                              \"maxplaytime\",\n                              \"minplaytime\",\n                              \"minage\",\n                              \"mechanics\",\n                              \"categories\")\n  ) |&gt;\n  step_playingtime_and_playercount() |&gt;\n  step_dummy_playercounts(threshold = 50) |&gt;\n  step_count_items(mechanics, categories) |&gt;\n  add_dummies(mechanics, threshold = 15) |&gt;\n  add_dummies(categories, threshold = 15) |&gt;\n  add_zv() |&gt;\n  add_normalize()\n\n# recipe also adding in families components mechanisms\nfull_recipe = \n  split |&gt;\n  training() |&gt;\n  build_recipe(outcome = interaction,\n               predictors = c(\"minplayers\", \n                              \"maxplayers\",\n                              \"playingtime\",\n                              \"maxplaytime\",\n                              \"minplaytime\",\n                              \"minage\",\n                              \"mechanics\",\n                              \"categories\",\n                              \"families\", \n                              \"mechanisms\",\n                              \"components\",\n                              \"themes\")\n  ) |&gt;\n  step_playingtime_and_playercount() |&gt;\n  step_dummy_playercounts(threshold = 50) |&gt;\n  step_count_items(mechanics, categories) |&gt;\n  add_dummies(mechanics, threshold = 15) |&gt;\n  add_dummies(categories, threshold = 15) |&gt;\n  add_dummies(families, threshold = 15) |&gt;\n  add_dummies(mechanisms, threshold = 15) |&gt;\n  add_dummies(components, threshold = 15) |&gt;\n  add_dummies(themes, threshold = 15) |&gt;\n  add_zv() |&gt;\n  add_normalize()\n\n# pca with full\npca_recipe = \n  full_recipe |&gt;\n  step_pca(all_numeric_predictors(), id = \"pca\", threshold = .50)\n\n# pls feature reduction\npls_recipe = \n  full_recipe |&gt;\n  step_pls(outcome = \"interaction\", all_numeric_predictors(), num_comp = 4)\n\n\nJust to illustrate, I’ll take one of these recipes and inspect how interaction varies along with some of the features we have. Looking first at numeric features via a scatterplot.\n\n\nShow the code\nbaked = \n  expanded_recipe |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nbaked |&gt;\n  select(-starts_with(\"mechanics_\"), -starts_with(\"categories\")) |&gt;\n  pivot_longer(\n    cols = -c(game_id, name, yearpublished, interaction),\n    names_to = c(\"feature\"),\n    values_to = c(\"value\")\n  ) |&gt;\n  ggplot(aes(x=interaction, y=value))+\n  geom_point(size = 0.5)+\n  facet_wrap(feature ~., scales = \"free\")+\n  geom_smooth(method = 'loess', formula = 'y~x')\n\n\n\n\n\n\n\n\n\nNext looking at mechanics. As we would expect, there are certain mechanics that are a dead giveaway that a game will be more interactive, such as Take-That and Area Majority, while other mechanics indicate a game is more of a Euro and less interactive, such as Worker/Tile Placement. This makes me wonder whether a model trained to predict interactivity is really just going to be picking up on the differences between “American” (thematic, conflict) and “European” (economy, efficiency) schools of design?\n\n\nShow the code\npivot_categorical = function(data) {\n  \n  data |&gt;\n    pivot_longer(\n      cols = -c(game_id, name, yearpublished, interaction),\n      names_to = c(\"feature\"),\n      values_to = c(\"value\")\n    ) |&gt;\n    mutate(value = case_when(value == 1 ~ 'yes', TRUE ~ 'no')) \n  \n}\n\nplot_categorical = function(data, ncol = 5) {\n  \n  data |&gt;\n    ggplot(aes(x=interaction, y=value, color = value))+\n    # geom_boxplot()+\n    geom_point(size = 0.5, alpha = 0.25, position = ggforce::position_jitternormal(sd_x = 0))+\n    geom_boxplot(alpha = 0.5, outliers = F)+\n    facet_wrap(~bggUtils::present_bgg_text(feature), scales = \"free\", ncol = ncol)+\n    theme(strip.text.x = element_text(size = 6))+\n    guides(color = 'none')+\n    ylab(\"feature\")+\n    scale_color_viridis_d(begin = 0.8, end = 0) +\n    scale_fill_viridis_d(begin = 0.8, end = 0)\n}\n\nboxplot_categorical = function(data) {\n  \n  data |&gt;\n    filter(value == 'yes') |&gt;\n    mutate(feature = bggUtils::present_bgg_text(feature)) |&gt;\n    group_by(feature) |&gt;\n    mutate(mean_rating = mean(interaction)) |&gt;\n    ungroup() |&gt;\n    ggplot(aes(x=interaction, y=reorder(feature, mean_rating)))+\n    geom_boxplot(alpha = 0.5, outliers = F)+\n    geom_point(size = 0.1, alpha = 0.5, position = ggforce::position_jitternormal(sd_x = 0))+\n    ylab(\"\")+\n    xlab(\"interaction rating\")\n  \n}\n\ndotplot_categorical = function(data) {\n  \n  data |&gt;\n    group_by(feature, value) |&gt; \n    summarize(mean = mean(interaction)) |&gt; \n    mutate(feature = bggUtils::present_bgg_text(feature)) |&gt;\n    ggplot(aes(x=mean, y=reorder(feature, mean)))+\n    geom_line(aes(group = feature), color = 'grey60')+\n    geom_point(aes(color = value))+\n    ylab(\"feature\")+\n    xlab(\"mean interaction rating\")\n  \n}\n\n\nbaked |&gt;\n  select(game_id, name, yearpublished, interaction, starts_with(\"mechanics_\")) |&gt;\n  pivot_categorical() |&gt;\n  mutate(feature = gsub(\"^mechanics_\", \"\", feature)) |&gt;\n  boxplot_categorical()+\n  theme_light()+\n  labs(title = \"Interactivity by Mechanic\")\n\n\n\n\n\n\n\n\n\nWe notice something similar if we look at categories, with Fantasy and Fighting games tending to be more interactive than Economic games. Here we can also notice that certain categories that we would associate with party/social deduction games (Bluffing, Deduction) tend to have higher player interaction.\n\n\nShow the code\nbaked |&gt;\n  select(game_id, name, yearpublished, interaction, starts_with(\"categories_\")) |&gt;\n  pivot_categorical() |&gt;\n  mutate(feature = gsub(\"^categories_\", \"\", feature)) |&gt;\n  boxplot_categorical()+\n  theme_light()+\n  labs(title = \"Interactivity by Category\")\n\n\n\n\n\n\n\n\n\nI created a specific recipe just to deal with playercounts - I’m thinking that the best way to handle player counts might be to simply create dummies for individual player counts rather than just use the minimum and maximum player count.\n\n\nShow the code\nplayercounts_recipe |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  select(game_id, name, interaction, starts_with(\"player_counts\")) |&gt;\n  pivot_longer(cols = starts_with(\"player_counts\"),\n               names_to = c(\"feature\")) |&gt;\n  mutate(value = case_when(value &gt; 0 ~ 'yes',\n                           value &lt; 0 ~ 'no')) |&gt;\n  plot_categorical(ncol = 2)"
  },
  {
    "objectID": "exploratory.html#results",
    "href": "exploratory.html#results",
    "title": "Analyzing Interactivity",
    "section": "3.1 Results",
    "text": "3.1 Results\nI’ll now examine how each of these workflows performed during resampling.\n\n\nShow the code\nset.seed(1999)\nmodels = \n  wflows |&gt;\n  workflow_map(fn = \"tune_grid\",\n               resamples = folds,\n               verbose = T,\n               grid = 15,\n               metrics = metric_set(rmse, rsq, mape, ccc),\n               control = control_grid(save_pred = T, save_workflow = T)\n  )\n\n\nHow did they all do? I’ll just grab the resulting performance for every candidate model and plot it for each metric.\n\n\nShow the code\nmodels |&gt;\n  autoplot(type = \"wflow_id\")+\n  theme_light()+\n  theme(legend.position = 'top',\n        legend.title = element_blank(),\n        legend.text = element_text(size = 6))+\n  scale_color_viridis_d(option = 'B')\n\n\n\n\n\n\n\n\n\nNotice that there are more workflows shown in the plot than the ones that I fit - this is because for each of the glmnet models we are further evaluating a set of candidate models over its tuning parameter.\nI’ll just extract the best performing model for each workflow and see how it performs across these metrics. The best models, so far, are a simple linear model that uses partial least squares for feature extraction, followed by a ridge regression that uses all features + playercount dummies.\n\n\nShow the code\nmodels |&gt; \n  rank_results(select_best = T, rank_metric = \"ccc\") |&gt;\n  select(wflow_id, .metric, mean) |&gt;\n  pivot_wider(names_from = c(\".metric\"),\n              values_from = c(\"mean\")) |&gt;\n  gt_tbl()|&gt;\n  gt::fmt_number(columns = c(\"ccc\", \"mape\", \"rmse\", \"rsq\"), decimals = 3)\n\n\n\n\n\n  \n    \n      wflow_id\n      ccc\n      mape\n      rmse\n      rsq\n    \n  \n  \n    pls_lm\n0.705\n17.112\n0.633\n0.539\n    full+playercounts_ridge\n0.700\n17.491\n0.642\n0.530\n    full+playercounts_lasso\n0.698\n17.671\n0.645\n0.527\n    full+playercounts_lm\n0.693\n17.997\n0.662\n0.513\n    expanded+playercounts_ridge\n0.683\n17.639\n0.650\n0.511\n    expanded+playercounts_lasso\n0.681\n17.739\n0.653\n0.509\n    expanded+playercounts_lm\n0.679\n17.956\n0.665\n0.498\n    expanded_lm\n0.650\n18.693\n0.689\n0.463\n    expanded_lasso\n0.649\n18.693\n0.688\n0.463\n    pca_lm\n0.633\n18.030\n0.656\n0.487\n    expanded_ridge\n0.631\n18.904\n0.689\n0.448\n    baseline_lm\n0.406\n22.205\n0.790\n0.259\n  \n  \n  \n\n\n\n\nI’ll extract predictions from the models and compare the predictions to the actual.\n\n\nShow the code\ncollect_best = function(obj) {\n  \n  best_tuned = \n    obj |&gt;\n    mutate(best_tune = map(result, ~ show_best(.x, metric = 'ccc'))) |&gt;\n    select(wflow_id, best_tune) |&gt;\n    unnest(best_tune)\n  \n  # \n  preds =\n    obj |&gt;\n    collect_predictions(summarize = T)\n  \n  preds |&gt;\n    inner_join(best_tuned)\n  \n}\n\nplot_predictions = function(data) {\n  \n  data |&gt;\n    ggplot(aes(x=.pred, y=interaction, label = name))+\n    geom_point(size = 1.5)+\n    geom_text(size = 1.5, vjust = 1.5)+\n    coord_cartesian(xlim = c(0.5, 5.5), ylim = c(0.5, 5.5))+\n    geom_abline(slope = 1,\n                linetype = 'dashed')+\n    xlab(\"estimated interactivity\")+\n    ylab(\"actual interactivity\")+\n    ggpubr::stat_cor()\n  \n}\n\nmodels |&gt;\n  collect_best() |&gt;\n  left_join(\n    split |&gt;\n      training() |&gt;\n      mutate(.row = row_number()) |&gt;\n      select(game_id, name, .row)\n  ) |&gt;\n  select(wflow_id, game_id, name, .row, .pred, interaction) |&gt;\n  ggplot(aes(x=.pred, y=interaction))+\n  geom_point(size = 0.5)+\n  facet_wrap(~wflow_id)+\n  theme_light()+\n  ggpubr::stat_cor()+\n  geom_abline(slope = 1, linetype = 'dashed')\n\n\n\n\n\n\n\n\n\nAt this point, the best models look to be the penalized regressions and, interestingly, the linear model that used partial least squares as a dimension reduction method. I should explore that some more."
  },
  {
    "objectID": "exploratory.html#sparsity-and-features",
    "href": "exploratory.html#sparsity-and-features",
    "title": "Analyzing Interactivity",
    "section": "3.2 Sparsity and Features",
    "text": "3.2 Sparsity and Features\nI’m curious to see sparse how partial least squares with a really rich set of features can do. Part of the problem with using categorical features in this setting is that many categories are quite sparse, which leads to dummy variables with mostly zeroes. For instance, while it might be useful to know that a game has a ‘Prisoner’s Dilemma’ mechanic for the purpose of measuring interactivity, there might only be one game in the dataset that has this feature, which make it difficult for a model to find an effect for this feature.\nJust looking at the games in this sample, we have over 100 different mechanics, the vast majority of which are only present in a handful of games. I’m generally not a fan of treemap charts, but I think it conveys the idea.\n\n\nShow the code\nrank_categorical = function(data, min = 1, breaks = 3) {\n  \n  data |&gt;\n    mutate(value = forcats::fct_lump_min(value, min = min)) |&gt;\n    group_by(type, value) |&gt;\n    count() |&gt;\n    ungroup() |&gt;\n    add_page_rank(breaks = breaks) |&gt;\n    mutate(page_rank = factor(page_rank))\n  \n}\n\nadd_page_rank = function(data, breaks = 5, ties = \"min\") {\n  \n  if (breaks &gt; 1) {\n    \n    tmp = data |&gt;\n      mutate(rank = rank(-n, ties = ties),\n             rank_cut = cut(rank, breaks = breaks, labels = F)) |&gt;\n      group_by(rank_cut) |&gt;\n      mutate(page_rank = paste(min(rank), max(rank), sep = \"-\")) |&gt;\n      ungroup() |&gt;\n      arrange(rank)\n    \n  } else if (breaks == 1) {\n    tmp = data |&gt;\n      mutate(rank = rank(-n, ties = ties),\n             page_rank =  paste(min(rank), max(rank), sep = \"-\")) |&gt;\n      arrange(rank)\n  }\n  \n  tmp |&gt;\n    mutate(page_rank = factor(page_rank, levels = unique(tmp$page_rank)))\n  \n}\n\nplot_ranks = function(data, ...) {\n  \n  data |&gt; \n    ggplot(aes(y=reorder(value,n),\n               x=n,\n               text = paste(\n                 paste(value),\n                 paste(\"rank:\", rank),\n                 paste(\"games:\", n),\n                 sep = \"\\n\")))+\n    geom_col() +\n    ylab(\"\")+\n    facet_wrap(page_rank~., scales = \"free_y\", ...)\n}\n\nplot_treemap = function(data, ...) {\n  \n  data |&gt;\n    treemap(\n      index=c(\"type\", \"value\"),\n      vSize=\"n\",\n      vColor = \"family_value\",\n      algorithm = \"pivotSize\",\n      mirror.y = TRUE,\n      mirror.x = TRUE,\n      border.lwds = 0.7,\n      aspRatio = 5/3,\n      palette = \"Paired\",\n      border.col = \"white\",\n      ...\n    )\n}\n\n\ngames_raw |&gt; \n  bggUtils:::unnest_mechanics() |&gt;\n  inner_join(\n    games_and_interaction |&gt;\n      select(game_id, name, yearpublished, interaction),\n    by = join_by(game_id)\n  ) |&gt;\n  rank_categorical(min= 1, breaks = 2) |&gt;\n  plot_treemap(title = \"Mechanics\")\n\n\n\n\n\n\n\n\n\nAnd there’s also really interesting info to be gleaned from BGG families, such as components, mechanisms, countries, but there are so many of these that it’s a challenge to even visualize. From the couple hundred games for which we have an interaction rating, the following plot displays all of the different types of BGG family variables that are present.\n\n\nShow the code\ngames_raw |&gt; \n  bggUtils:::unnest_families() |&gt;\n  inner_join(\n    games_and_interaction |&gt;\n      select(game_id, name, yearpublished, interaction),\n    by = join_by(game_id)\n  ) |&gt;\n  rename(family = type) |&gt; \n  separate(value, into = c(\"type\", \"value\"), sep = \": \") |&gt;\n  rank_categorical(min= 1, breaks = 2) |&gt;\n  plot_treemap(title = \"BGG Families\")\n\n\n\n\n\n\n\n\n\nZoom into just a few of these a bit more, we can probably spot that some components and mechanisms that we would expect to be useful.\n\n\nShow the code\ngames_raw |&gt; \n  bggUtils:::unnest_families() |&gt;\n  inner_join(\n    games_and_interaction |&gt;\n      select(game_id, name, yearpublished, interaction),\n    by = join_by(game_id)\n  ) |&gt;\n  rename(family = type) |&gt; \n  separate(value, into = c(\"type\", \"value\"), sep = \": \") |&gt;\n  filter(type %in% c(\"Mechanism\", \"Components\", \"Country\", \"Players\")) |&gt;\n  rank_categorical(min= 1, breaks = 2) |&gt;\n  plot_treemap(title = \"\")\n\n\n\n\n\n\n\n\n\nI’m going to try out a a recipe that allows for almost any of these as possible features into a model, and trust that a method like partial least squares can handle it. Basically this method first attempts to reduce the feature space by finding linear combinations of variables that maximize covariance with the outcome. It’s similar-ish to PCA, but PCA but is purely unsupervised in aiming to find components that maximize variance. PLS is a supervised dimension reduction method, aiming to find latent variables that are associated with an outcome.\nSetting up the recipe and workflow.\n\n\nShow the code\nbuild_sparse_recipe = function(data, predictors, threshold = 1) {\n  \n  data |&gt;\n    build_recipe(outcome = interaction,\n                 predictors = c(\"minplayers\", \n                                \"maxplayers\",\n                                \"playingtime\",\n                                \"maxplaytime\",\n                                \"minplaytime\",\n                                \"minage\",\n                                \"mechanics\",\n                                \"categories\",\n                                \"families\", \n                                \"mechanisms\",\n                                \"components\",\n                                \"themes\")\n    ) |&gt;\n    step_playingtime_and_playercount() |&gt;\n    step_dummy_playercounts(threshold = threshold) |&gt;\n    step_count_items(mechanics, categories) |&gt;\n    add_dummies(mechanics, threshold = threshold) |&gt;\n    add_dummies(categories, threshold = threshold) |&gt;\n    add_dummies(families, threshold = threshold) |&gt;\n    add_dummies(mechanisms, threshold = threshold) |&gt;\n    add_dummies(components, threshold = threshold) |&gt;\n    add_dummies(themes, threshold = threshold) |&gt;\n    add_zv() |&gt;\n    add_normalize()\n  \n}\n\nsparse_recipe = \n  split |&gt;\n  training() |&gt;\n  build_sparse_recipe()\n\n\nI’ll then tune this one over the number of components and proportion of predictors to include.\n\n\nShow the code\nsparse_wflow = \n  workflow() |&gt;\n  add_model(\n    pls_mod\n  ) |&gt;\n  add_recipe(\n    sparse_recipe\n  )\n\nsparse_tuned = \n  sparse_wflow |&gt;\n  tune_grid(\n    grid = 15,\n    resamples = folds,\n    metrics = metric_set(rmse, rsq, mape, ccc),\n    control = control_resamples(save_pred = T, save_workflow = T)\n  )\n\n\nNow extracting the results from tuning; looks like four components with a low proportion of predictors was our best result.\n\n\nShow the code\nsparse_tuned |&gt;\n  collect_metrics(type = 'wide') |&gt;\n  mutate_if(is.numeric, round, 3) |&gt;\n  arrange(desc(ccc)) |&gt;\n  gt_tbl()\n\n\n\n\n\n  \n    \n      predictor_prop\n      num_comp\n      .config\n      ccc\n      mape\n      rmse\n      rsq\n    \n  \n  \n    0.214\n4\nPreprocessor1_Model04\n0.747\n15.450\n0.579\n0.604\n    0.500\n4\nPreprocessor1_Model08\n0.743\n15.179\n0.575\n0.610\n    0.429\n3\nPreprocessor1_Model07\n0.736\n15.321\n0.578\n0.606\n    0.357\n2\nPreprocessor1_Model06\n0.725\n15.637\n0.578\n0.607\n    0.143\n2\nPreprocessor1_Model03\n0.724\n15.905\n0.591\n0.581\n    0.786\n4\nPreprocessor1_Model12\n0.722\n15.629\n0.586\n0.598\n    0.714\n3\nPreprocessor1_Model11\n0.717\n15.684\n0.585\n0.602\n    0.643\n2\nPreprocessor1_Model10\n0.711\n15.827\n0.582\n0.610\n    1.000\n3\nPreprocessor1_Model15\n0.700\n16.107\n0.595\n0.590\n    0.929\n2\nPreprocessor1_Model14\n0.690\n16.327\n0.594\n0.600\n    0.286\n1\nPreprocessor1_Model05\n0.675\n17.022\n0.620\n0.551\n    0.071\n1\nPreprocessor1_Model02\n0.662\n17.699\n0.635\n0.519\n    0.571\n1\nPreprocessor1_Model09\n0.652\n17.368\n0.626\n0.547\n    0.857\n1\nPreprocessor1_Model13\n0.629\n17.894\n0.640\n0.528\n    0.000\n3\nPreprocessor1_Model01\n0.339\n24.387\n0.848\n0.174\n  \n  \n  \n\n\n\n\nHow do the predictions from this method compare to what we found earlier?\n\n\nShow the code\nsparse_tuned |&gt;\n  collect_predictions(summarize = T) |&gt;\n  inner_join(\n    sparse_tuned |&gt;\n      select_best(metric = 'ccc')\n  ) |&gt;\n  arrange(.row) |&gt;\n  inner_join(\n    games_and_interaction |&gt;\n      mutate(.row = row_number()) |&gt;\n      select(game_id, name, .row)\n  ) |&gt;\n  plot_predictions()"
  },
  {
    "objectID": "exploratory.html#inference",
    "href": "exploratory.html#inference",
    "title": "Analyzing Interactivity",
    "section": "3.3 Inference",
    "text": "3.3 Inference\nOkay so we’ve fit a decent number of models at this point, some of which look to be on the right track.\nOne of the best models at this stage is the ridge regression with player count dummies. I’ll fit this model to the entirety of the training set to grab the coefficients and get a sense of what it’s learning from the data. I’ll also add a model postprocessor to constrain predictions between 1 and 5.\nLooking at the coefficients, we can see (as before) that certain families (Two Player Only), mechanics (Take That, Area Majority), categories (Deduction, Territory Building), and player counts (5 players) are associated with higher interactivity ratings. Mechanics/mechanisms associated with Euro style games (End Game Bonuses, Tableau Building, Tile Placement) and low player counts (x2, x3) tend to reduce interactivity.\n\n\nShow the code\n# fit \nridge_fit = \n  models |&gt;\n  filter(wflow_id == 'full+playercounts_ridge') |&gt;\n  fit_best(metric = \"ccc\")  |&gt;\n  add_tailor(\n    tailor() |&gt;\n      adjust_numeric_range(lower_limit = 1, upper_limit = 5)\n  )\n\n# extract coefs\nridge_fit |&gt;\n  tidy() |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  slice_max(abs(estimate), n = 35) |&gt;\n  mutate(sign = case_when(estimate &gt; 0 ~ 'increases interactivity',\n                          estimate &lt; 0 ~ 'decreases interactivity')) |&gt;\n  mutate(tidy_term = bggUtils::present_bgg_text(term)) |&gt;\n  ggplot(aes(y=reorder(tidy_term, estimate),\n             x=estimate))+\n  geom_col()+\n  ylab(\"\")+\n  facet_wrap(~sign, scales = \"free_x\")+\n  theme(axis.text.y = element_text(size = 6))\n\n\n\n\n\n\n\n\n\nNow I’ll take a look at the partial least squares model. It’s a bit trickier to extract info from these, but basically we can get a sense of how individual features map to the latent variables (components), and we can also plot the components themselves compared to the outcome.\n\n\nShow the code\nsparse_fit =\n  sparse_tuned |&gt;\n  fit_best(metric = \"ccc\")  |&gt;\n  add_tailor(\n    tailor() |&gt;\n      adjust_numeric_range(lower_limit = 1, upper_limit = 5)\n  )\n\n\nFirst, look at how variables map to the loadings. I’ll grab the top variables for each component - the scores themselves only have a directional interpretation, showing how different variables affect the\n\n\nShow the code\nget_loadings = function(obj) {\n  \n  obj |&gt;\n    pluck(\"loadings\", 1) |&gt; \n    as.data.frame() |&gt; \n    rownames_to_column(\"feature\") |&gt; \n    as_tibble()\n  \n}\n\nsparse_fit |&gt;\n  extract_fit_engine() |&gt; \n  get_loadings() |&gt;\n  pivot_longer(cols = starts_with(\"comp\")) |&gt;\n  group_by(name) |&gt;\n  slice_max(n = 25, order_by = abs(value)) |&gt;\n  mutate(feature = bggUtils::present_bgg_text(feature),\n         feature = gsub(\"Themes Theme\", \"Theme\", feature)) |&gt;\n  ggplot(aes(x=value, y=tidytext::reorder_within(feature, abs(value), name)))+\n  geom_col()+\n  facet_wrap(name ~., scales = \"free_y\")+\n  tidytext::scale_y_reordered()+\n  ylab(\"\")+\n  theme(axis.text.y = element_text(size = 6),\n        panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\n\nNext, the latent variables. I’ll extract these and plot them against the interaction rating.\n\n\nShow the code\nsparse_fit |&gt;\n  extract_fit_engine() |&gt;\n  pluck(\"variates\", 1) |&gt;\n  bind_cols(\n    split |&gt; \n      training() |&gt;\n      select(game_id, name, yearpublished, interaction)\n  ) |&gt;\n  pivot_longer(\n    cols = starts_with(\"comp\"),\n    names_to = c(\"component\")\n  ) |&gt;\n  ggplot(aes(x=interaction, y=value, label = name))+\n  geom_point(alpha = 0.5, size = 1.5)+\n  geom_text(size = 1.5, vjust = 1, check_overlap = T)+\n  facet_wrap(component ~., scales = \"free\")\n\n\n\n\n\n\n\n\n\nDefinitely seeing some wonky stuff with games like Hive, Go, Patchwork; I’ll need to dig into that some more. But that first component, man that looks almost exactly like ranking games from a continuum from ‘multiplayer solitaire’ to ‘talking to people’. Really interesting."
  },
  {
    "objectID": "exploratory.html#validation",
    "href": "exploratory.html#validation",
    "title": "Analyzing Interactivity",
    "section": "3.4 Validation",
    "text": "3.4 Validation\nI’ll take the ridge and partial least squares and evalaute their performance on the validation set. We’d like to see our estimated interactive rating be pretty close to the actual here; I want to see which approach does a better job on the validation set.\n\n\nShow the code\nvalid_preds = \n  map(list(\"pls\" = sparse_fit, \"ridge\" = ridge_fit),\n      ~ .x |&gt; \n        augment(split |&gt; testing())\n  ) |&gt;\n  bind_rows(.id = 'model') |&gt;\n  select(model, .pred, game_id, name, interaction)\n\nvalid_preds |&gt; \n  ggplot(aes(x=.pred, y=interaction, label = name))+\n  geom_point(size = 1.5)+\n  geom_text(size = 1.5, vjust = 1.5)+\n  coord_cartesian(xlim = c(0.5, 5.5), ylim = c(0.5, 5.5))+\n  geom_abline(slope = 1,\n              linetype = 'dashed')+\n  xlab(\"estimated interactivity\")+\n  ylab(\"actual interactivity\")+\n  ggpubr::stat_cor()+\n  facet_wrap(model ~.)\n\n\n\n\n\n\n\n\n\nThe biggest misses for both models look to be Railroad Ink and Troyes. I’d guess that the model thinks Railroad Ink has more interaction than it does because it has a low play time and can play at higher playercounts? Troyes I can see a model failing to pick up on - on paper it’s a dry Euro game, but it’s actually quite interactive because of you can block and buy each other’s dice.\nWhere do they most disagree? I’ll compare their predictions side by side.\n\n\nShow the code\nvalid_preds |&gt;\n  pivot_wider(names_from = c(\"model\"), values_from = c(\".pred\")) |&gt;\n  ggplot(aes(x=pls, y=ridge, label = name))+\n  geom_point(size = 2, alpha = 0.5)+\n  geom_text(check_overlap = T, vjust = -1, size = 1.5)+\n  geom_abline(slope = 1, linetype = 'dotted')\n\n\n\n\n\n\n\n\n\nI wonder if an ensemble would be the next step?\nAt any rate, each model’s performance on the validation set is in line with what we found during resampling, which should alleviate fears of overfitting on such a small dataset.\n\n\nShow the code\nmetrics = metric_set(rmse, rsq, mape, ccc)\n\nvalid_preds |&gt;\n  group_by(model) |&gt;\n  metrics(\n    interaction,\n    .pred\n  ) |&gt; \n  pivot_wider(names_from = c(\".metric\"), values_from = c(\".estimate\")) |&gt;\n  mutate_if(is.numeric, round, 3) |&gt;\n  gt_tbl()\n\n\n\n\n\n  \n    \n      model\n      .estimator\n      rmse\n      rsq\n      mape\n      ccc\n    \n  \n  \n    pls\nstandard\n0.576\n0.602\n16.501\n0.765\n    ridge\nstandard\n0.588\n0.586\n16.826\n0.757\n  \n  \n  \n\n\n\n\nRight now, the leader in the clubhouse is the partial least squares model, which is not something I was expecting going in. I’ll fit that model to repeated resamples of the training set to get out of sample estimates for every game for which we do have a rating.\n\n\nShow the code\nbest_mod = \n  models |&gt;\n  bind_rows(as_workflow_set(sparse_pls = sparse_tuned)) |&gt;\n  fit_best(metric = 'ccc') |&gt;\n  add_tailor(\n    tailor() |&gt;\n      adjust_numeric_range(lower_limit = 1, upper_limit = 5)\n  )\n\n\nset.seed(1999)\nfolds_all = split$data |&gt; vfold_cv(strata = interaction, repeats = 3)\n\noos_preds = \n  best_mod |&gt;\n  fit_resamples(resamples = folds_all,\n                control = control_resamples(save_pred = T))\n\noos_estimates = \n  oos_preds |&gt;\n  collect_predictions(summarize = T) |&gt;\n  select(.row, .pred) |&gt;\n  left_join(\n    split$data |&gt;\n      mutate(.row = row_number()) |&gt;\n      select(.row, game_id, name, yearpublished, interaction)\n  )\n\n\n\n\nShow the code\noos_estimates |&gt;\n  plot_predictions()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nestimates_tbl = function(data){\n  \n  data |&gt;\n    arrange(desc(interaction)) |&gt;\n    gt_tbl() |&gt;\n    gt::data_color(columns = c(\"estimate\", \"interaction\"),\n                   method = \"numeric\",\n                   palette = \"viridis\",\n                   domain = c(1, 5),\n                   reverse = T) |&gt;\n    gt::cols_align(columns = c(\"estimate\", \"interaction\"),\n                   align = \"center\") |&gt;\n    gt::opt_interactive()\n}\n\n\noos_estimates |&gt;\n  mutate_if(is.numeric, round, 3) |&gt;\n  select(game_id, name, estimate = .pred, interaction) |&gt;\n  arrange(desc(interaction)) |&gt;\n  mutate(game_id = as.character(game_id)) |&gt;\n  estimates_tbl() |&gt;\n  gt::cols_width(\n    game_id ~ px(75)\n  )"
  },
  {
    "objectID": "exploratory.html#top-250",
    "href": "exploratory.html#top-250",
    "title": "Analyzing Interactivity",
    "section": "4.1 Top 250",
    "text": "4.1 Top 250\nLet’s filter to only games in the top 250 of BGG’s geek rating, which will make this easier to see. Let’s look at them based on interaction vs complexity. How does this map to my expectations?\n\n\nShow the code\ndf = \n  estimates |&gt;\n  rename(interaction = .pred) |&gt;\n  slice_max(bayesaverage, n = 250)\n\nplot_interaction_vs_complexity = function(data) {\n  \n  data |&gt;\n    ggplot(aes(x=averageweight, y=interaction, label = name))+\n    #geom_point(size = 2, alpha = 0.25, position = ggforce::position_jitternormal())+\n    geom_text(check_overlap = T, size = 2)+\n    ylab(\"estimated interaction\")+\n    geom_vline(xintercept = median(df$averageweight), linetype = 'dashed')+\n    geom_hline(yintercept = median(df$interaction), linetype = 'dashed')+\n    coord_cartesian(ylim = c(0.8, 5.2), xlim = c(0.8, 5.2))+\n    annotate(geom = \"label\", y = 4.8, x=1.2, label = \"low complexity \\n high interaction\", size = 2)+\n    annotate(geom = \"label\", y = 1.2, x=4.6, label = \"high complexity \\n low interaction\", size = 2)+\n    annotate(geom = \"label\", y = 4.8, x=4.6, label = \"high complexity \\n high interaction\", size = 2)+\n    annotate(geom = \"label\", y = 1.2, x=1.2, label = \"low complexity \\n low interaction\", size = 2)\n}\n\ndf |&gt;\n  plot_interaction_vs_complexity()\n\n\n\n\n\n\n\n\n\nHonestly this looks pretty good to my eye.\n\n\nShow the code\ntop_games = \n  estimates |&gt;\n  rename(interaction = .pred) |&gt;\n  slice_max(bayesaverage, n = 250) |&gt;\n  select(game_id, name, geek_rating = bayesaverage, averageweight, interaction) |&gt;\n  mutate_if(is.numeric, round, 3) |&gt;\n  arrange(interaction)\n\n\nWhich games in the top 250 are considered the least interactive? This looks to mostly be Euros and roll and writes that lean towards multiplayer solitaire.\n\n\nShow the code\ntop_games |&gt;\n  arrange(interaction) |&gt;\n  head(25) |&gt;\n  gt_tbl() |&gt;\n  gt::data_color(columns = c(\"interaction\"),\n                 method = \"numeric\",\n                 palette = \"viridis\",\n                 domain = c(1, 5),\n                 reverse = T) |&gt;\n  gt::cols_align(columns = c(\"interaction\", \"geek_rating\"),\n                 align = \"center\")\n\n\n\n\n\n  \n    \n      game_id\n      name\n      geek_rating\n      averageweight\n      interaction\n    \n  \n  \n    233867\nWelcome To...\n7.374\n1.840\n1.000\n    263918\nCartographers\n7.418\n1.877\n1.286\n    339789\nWelcome to the Moon\n7.356\n2.479\n1.444\n    266192\nWingspan\n7.888\n2.468\n1.667\n    199561\nSagrada\n7.332\n1.917\n1.759\n    271320\nThe Castles of Burgundy\n7.945\n2.922\n1.778\n    123260\nSuburbia\n7.302\n2.763\n1.804\n    102680\nTrajan\n7.482\n3.635\n1.806\n    295947\nCascadia\n7.764\n1.851\n1.816\n    84876\nThe Castles of Burgundy\n8.016\n2.978\n1.879\n    342942\nArk Nova\n8.338\n3.770\n1.909\n    304783\nHadrian's Wall\n7.418\n3.157\n1.920\n    203993\nLorenzo il Magnifico\n7.511\n3.299\n1.969\n    350184\nEarth\n7.360\n2.881\n1.982\n    266524\nPARKS\n7.432\n2.120\n2.001\n    364011\nGreat Western Trail: Argentina\n7.325\n3.885\n2.002\n    341169\nGreat Western Trail: Second Edition\n7.906\n3.720\n2.028\n    185343\nAnachrony\n7.753\n4.007\n2.038\n    244522\nThat's Pretty Clever!\n7.379\n1.861\n2.049\n    182874\nGrand Austria Hotel\n7.698\n3.213\n2.069\n    266507\nClank! Legacy: Acquisitions Incorporated\n7.947\n2.732\n2.071\n    371942\nThe White Castle\n7.498\n3.023\n2.076\n    365717\nClank!: Catacombs\n7.574\n2.514\n2.082\n    177736\nA Feast for Odin\n7.944\n3.859\n2.084\n    266810\nPaladins of the West Kingdom\n7.675\n3.719\n2.116\n  \n  \n  \n\n\n\n\nWhich games are considered the most interactive? Wargames like Root, Rebellion, TI4, Twilight Struggle. Interactivity, in this case, is really picking up on the war/conflict/head to head side of things, though Five Tribes is one that’s a lot higher up there than I would have expected?\n\n\nShow the code\ntop_games |&gt;\n  arrange(desc(interaction)) |&gt;\n  head(25) |&gt;\n  gt_tbl() |&gt;\n  gt::data_color(columns = c(\"interaction\"),\n                 method = \"numeric\",\n                 palette = \"viridis\",\n                 domain = c(1, 5),\n                 reverse = T) |&gt;\n  gt::cols_align(columns = c(\"interaction\", \"geek_rating\"),\n                 align = \"center\")\n\n\n\n\n\n  \n    \n      game_id\n      name\n      geek_rating\n      averageweight\n      interaction\n    \n  \n  \n    37111\nBattlestar Galactica: The Board Game\n7.538\n3.249\n5.000\n    205896\nRising Sun\n7.472\n3.300\n5.000\n    283355\nDune\n7.351\n3.995\n5.000\n    103343\nA Game of Thrones: The Board Game (Second Edition)\n7.316\n3.729\n5.000\n    118\nModern Art\n7.315\n2.285\n4.939\n    103885\nStar Wars: X-Wing Miniatures Game\n7.383\n2.496\n4.922\n    187645\nStar Wars: Rebellion\n8.170\n3.741\n4.870\n    12333\nTwilight Struggle\n8.064\n3.610\n4.867\n    154597\nHive Pocket\n7.346\n2.245\n4.840\n    188\nGo\n7.313\n3.934\n4.817\n    240980\nBlood on the Clocktower\n7.445\n3.046\n4.742\n    25021\nSekigahara: The Unification of Japan\n7.337\n2.800\n4.723\n    237182\nRoot\n7.884\n3.810\n4.709\n    124742\nAndroid: Netrunner\n7.659\n3.410\n4.683\n    10630\nMemoir '44\n7.342\n2.274\n4.670\n    329082\nRadlands\n7.379\n2.246\n4.641\n    12493\nTwilight Imperium: Third Edition\n7.492\n4.258\n4.635\n    128882\nThe Resistance: Avalon\n7.323\n1.739\n4.602\n    171131\nCaptain Sonar\n7.304\n2.193\n4.595\n    14105\nCommands & Colors: Ancients\n7.305\n2.692\n4.591\n    233078\nTwilight Imperium: Fourth Edition\n8.236\n4.325\n4.585\n    39463\nCosmic Encounter\n7.336\n2.582\n4.581\n    42\nTigris & Euphrates\n7.516\n3.497\n4.539\n    155821\nInis\n7.539\n2.948\n4.523\n    164153\nStar Wars: Imperial Assault\n7.667\n3.303\n4.507"
  },
  {
    "objectID": "exploratory.html#top-1000",
    "href": "exploratory.html#top-1000",
    "title": "Analyzing Interactivity",
    "section": "4.2 Top 1000",
    "text": "4.2 Top 1000\nFinally, here’s the top 1000 games on BGG (just to make this table easier to) to sort through and examine along with other BGG ratings.\n\n\nShow the code\nestimates |&gt;\n  slice_max(bayesaverage, n = 1000) |&gt;\n  select(game_id, name, interaction = .pred, geek_rating = bayesaverage, averageweight, average) |&gt;\n  mutate_if(is.numeric, round, 3) |&gt;\n  gt_tbl() |&gt;\n  gt::opt_interactive() |&gt;\n  gt::data_color(columns = c(\"interaction\"),\n                 method = \"numeric\",\n                 palette = \"viridis\",\n                 domain = c(1, 5),\n                 reverse = T) |&gt;\n  gt::cols_align(columns = c(\"interaction\", \"geek_rating\", \"averageweight\"),\n                 align = \"center\") |&gt;\n  gt::opt_interactive()\n\n\n\n\n\n\n\n\n\nI’ll also write out these estimates and save them to a CSV.\n\n\nShow the code\nestimates |&gt;\n  select(game_id, name, interaction = .pred) |&gt;\n  write.csv(file = \"data/processed/estimates.csv\")"
  }
]