---
title: "Analyzing Interactivity"
format: 
  html:
    warning: false
    message: false
editor: source
execute:
  freeze: auto
  cache: true
---

# Data

<!-- # ```{r} -->
<!-- # #| include: false -->
<!-- # targets::tar_load_globals() -->
<!-- #  -->
<!-- # ``` -->
<!-- #  -->

```{r}
#| warning: false
#| message: false
#| 
# packages
library(tidyverse)
library(bggUtils)
library(ggforce)
library(gt)
library(gtExtras)

# functions
targets::tar_source("R")

# data
# survey responses
responses = targets::tar_read("responses")
interaction_table = targets::tar_read("interaction_table")

# games
games_raw = targets::tar_read("games_raw")
games_preprocessed = targets::tar_read("games_preprocessed")

# ggplot theme
theme_set(theme_bgg())

```

## Responses

The responses data has one record for each game from a respondent, with a rating ranging from 1 to 5. I can aggregate this to create a table with the number of votes for each rating, along with its mean and standard deviation.

```{r}

responses |>
  format_as_table() |>
  arrange(desc(n)) |>
  select(name, starts_with("rating_"), everything()) |>
  gt_ratings_tbl()

```

To see these games at a glance, I'll plot the proportion of votes for each rating for all games, ranking them from most interactive to least interactive.

```{r}
#| fig-height: 10
responses |>
  format_as_table() |>
  plot_responses_count()

```



```{r}

responses |>
  group_by(name) |>
  summarize_responses() |>
  plot_mean_vs_sd()

```

## Interaction Table

The responses document is slightly different than data shown in the `Interaction Table` sheet, which has more responses from, I'm guessing, previous surveys? Nonetheless this can be analyzed in a similar fashion.

```{r}

interaction_table |>
  gt_ratings_tbl()

```


```{r}
#| fig-height: 10
interaction_table |>
  plot_responses_count()

```

```{r}

interaction_table |>
  plot_mean_vs_sd()

```

This table has more data to work with, so I'm going to proceed using this rather than the raw responses, but this is something that can be easily flipped.

# Analysis

This survey gives us a measure of interactivity for a couple hundred games, but we'd really like to produce a rating for all games on BGG.

For instance, we have community measures from BGG regarding the complexity of games (average weight rating), along with a community's assessment of how *good* a game is (average user rating).

```{r}
#| message: false
#| warning: false
plot_bgg_ratings = 
  games_preprocessed |>
  ggplot(aes(x=averageweight, y=average))+
  geom_point(alpha = 0.25,size = 0.5, position = ggforce::position_jitternormal())+
  xlab("average weight")+
  ylab("average rating")

plot_bgg_ratings

```

I'll join up the dataset of all (ranked) BGG games with the games for which we have an interactivity rating. This is a much smaller sample, but I'll highlight these games on the previous graph. This highlights that the sample of games with an interactive rating are spread out pretty well across complexity, though most (understandably) are also games with a higher than average BGG rating. 

```{r}

plot_bgg_ratings +
  geom_point(data = games_preprocessed |>
               select(game_id, name, average, averageweight, usersrated) |>
               inner_join(interaction_table |> 
                            select(name, interactivity = mean)),
             color = 'blue')

```

## Methodology

How can we get an interactivity rating for games other than the ones shown in blue? The simplest answer is to just collect a bunch of data - get the community to respond to surveys and rate all of the games!

That's going to be difficult to accomplish, so my approach is to *estimate* the interactivity for all of these other games using the ratings we have in our sample. To do this, I will attempt to train a model to learn the relationship between attributues of games (as taken from BGG) and the interaction rating. If a model can learn how to predict this rating, we can then use the resulting model to estimate the interactive rating for games the community has not yet rated. These estimates can then be used to guide our labeling strategy for new games, which can then be fed back into the model, and so on.

This requires first assessing our ability to predict the interactive rating using information from BGG.

1. Train a model to predict interactivity as a function of BGG features.
2. Assess model performance via resampling and hold-out set.
3. Use model to predict games that do not have interactivity rating.
4. Use model predictions to guide labeling strategy for games without interactivity rating.
5. Go back to step 1 with expanded/updated interactivity rating
6. Repeat 1-4


```{r}
#| message: false
#| warning: false
targets::tar_load(interaction_estimates)

games_and_interaction = 
  games_preprocessed |>
  group_by(name) |>
  slice_max(usersrated, n =1) |>
  ungroup() |>
  inner_join(
    interaction_estimates
  )

```

We are in a very small data setting, which means we need to be careful in how we spend our data. Ultimately, the goal is to develop a model to predict the interactivity of games that do not have a rating, where we can trust that our model predicts this rating reasonably well. This means we need to assess how well a model can perform on unseen data.

To this end, I will create a train/validation split for games with interaction ratings. I will use the training set to develop a model and the validation set to assess its performance in predicting games not used in training the model.


```{r}
#| message: false
#| warning: false

library(tidymodels)
library(glmnet)
library(mixOmics)
library(tailor)
library(treemap)
library(plsmod)

suppressMessages({
  tidymodels::tidymodels_prefer()
})

set.seed(1999)
split = 
  games_and_interaction |>
  initial_split(strata = interaction)

set.seed(1999)
folds = 
  split |>
  training() |>
  vfold_cv(v = 5, strata = interaction, repeats = 3)

```

## Preprocessing

What might predict a game's interactive rating? I have a very rich set features for games, including everything from player counts, playing time, publishers, mechanics, categories, components, and so on. It would be tempting to take the kitchen sink approach and throw everything into a model and see what we get, but given that we have such a small dataset I would prefer to start with small set of features that we think would explain interactivity and add from there. 

My expectation is that player counts and playing time will play a role, along with certain types of mechanics and categories. I'll create a couple of different recipes, or preprocessors, that will generate different sets of features.

```{r}

# recipe with very minimal set of features features
baseline_recipe = 
  split |>
  training() |>
  build_recipe(outcome = interaction,
               predictors = c("minplayers", 
                              "maxplayers",
                              "playingtime",
                              "maxplaytime",
                              "minplaytime",
                              "minage")
  ) |>
  step_playingtime_and_playercount() |>
  # step_playercounts(threshold = 50) |>
  add_zv()

# expanded features; adding in mechanics and categoreis
expanded_recipe = 
  split |>
  training() |>
  build_recipe(outcome = interaction,
               predictors = c("minplayers", 
                              "maxplayers",
                              "playingtime",
                              "maxplaytime",
                              "minplaytime",
                              "minage",
                              "mechanics",
                              "categories")
  ) |>
  step_playingtime_and_playercount() |>
  step_count_items(mechanics, categories) |>
  add_dummies(mechanics, threshold = 15) |>
  add_dummies(categories, threshold = 15)

# normalize
norm_recipe = 
  expanded_recipe |>
  add_zv() |>
  add_normalize()


# trying out playercount dummies? idk
playercounts_recipe = 
  split |>
  training() |>
  build_recipe(outcome = interaction,
               predictors = c("minplayers", 
                              "maxplayers",
                              "playingtime",
                              "maxplaytime",
                              "minplaytime",
                              "minage",
                              "mechanics",
                              "categories")
  ) |>
  step_playingtime_and_playercount() |>
  step_dummy_playercounts(threshold = 50) |>
  step_count_items(mechanics, categories) |>
  add_dummies(mechanics, threshold = 15) |>
  add_dummies(categories, threshold = 15) |>
  add_zv() |>
  add_normalize()

# recipe also adding in families components mechanisms
full_recipe = 
  split |>
  training() |>
  build_recipe(outcome = interaction,
               predictors = c("minplayers", 
                              "maxplayers",
                              "playingtime",
                              "maxplaytime",
                              "minplaytime",
                              "minage",
                              "mechanics",
                              "categories",
                              "families", 
                              "mechanisms",
                              "components",
                              "themes")
  ) |>
  step_playingtime_and_playercount() |>
  step_dummy_playercounts(threshold = 50) |>
  step_count_items(mechanics, categories) |>
  add_dummies(mechanics, threshold = 15) |>
  add_dummies(categories, threshold = 15) |>
  add_dummies(families, threshold = 15) |>
  add_dummies(mechanisms, threshold = 15) |>
  add_dummies(components, threshold = 15) |>
  add_dummies(themes, threshold = 15) |>
  add_zv() |>
  add_normalize()

# pca with full
pca_recipe = 
  full_recipe |>
  step_pca(all_numeric_predictors(), id = "pca", threshold = .50)

# pls feature reduction
pls_recipe = 
  full_recipe |>
  step_pls(outcome = "interaction", all_numeric_predictors(), num_comp = 4)
```

Just to illustrate, I'll take one of these recipes and inspect how interaction varies along with some of the features we have. Looking first at numeric features via a scatterplot. 

```{r}

baked = 
  expanded_recipe |>
  prep() |>
  bake(new_data = NULL)

baked |>
  select(-starts_with("mechanics_"), -starts_with("categories")) |>
  pivot_longer(
    cols = -c(game_id, name, yearpublished, interaction),
    names_to = c("feature"),
    values_to = c("value")
  ) |>
  ggplot(aes(x=interaction, y=value))+
  geom_point(size = 0.5)+
  facet_wrap(feature ~., scales = "free")+
  geom_smooth(method = 'loess', formula = 'y~x')
```

Next looking at mechanics. As we would expect, there are certain mechanics that are a dead giveaway that a game will be more interactive, such as Take-That and Area Majority, while other mechanics indicate a game is more of a Euro and less interactive, such as Worker/Tile Placement. This makes me wonder whether a model trained to predict interactivity is really just going to be picking up on the differences between "American" (thematic, conflict) and "European" (economy, efficiency) schools of design?

```{r}

pivot_categorical = function(data) {
  
  data |>
    pivot_longer(
      cols = -c(game_id, name, yearpublished, interaction),
      names_to = c("feature"),
      values_to = c("value")
    ) |>
    mutate(value = case_when(value == 1 ~ 'yes', TRUE ~ 'no')) 
  
}

plot_categorical = function(data, ncol = 5) {
  
  data |>
    ggplot(aes(x=interaction, y=value, color = value))+
    # geom_boxplot()+
    geom_point(size = 0.5, alpha = 0.25, position = ggforce::position_jitternormal(sd_x = 0))+
    geom_boxplot(alpha = 0.5, outliers = F)+
    facet_wrap(~bggUtils::present_bgg_text(feature), scales = "free", ncol = ncol)+
    theme(strip.text.x = element_text(size = 6))+
    guides(color = 'none')+
    ylab("feature")+
    scale_color_viridis_d(begin = 0.8, end = 0) +
    scale_fill_viridis_d(begin = 0.8, end = 0)
}

boxplot_categorical = function(data) {
  
  data |>
    filter(value == 'yes') |>
    mutate(feature = bggUtils::present_bgg_text(feature)) |>
    group_by(feature) |>
    mutate(mean_rating = mean(interaction)) |>
    ungroup() |>
    ggplot(aes(x=interaction, y=reorder(feature, mean_rating)))+
    geom_boxplot(alpha = 0.5, outliers = F)+
    geom_point(size = 0.1, alpha = 0.5, position = ggforce::position_jitternormal(sd_x = 0))+
    ylab("")+
    xlab("interaction rating")
  
}

dotplot_categorical = function(data) {
  
  data |>
    group_by(feature, value) |> 
    summarize(mean = mean(interaction)) |> 
    mutate(feature = bggUtils::present_bgg_text(feature)) |>
    ggplot(aes(x=mean, y=reorder(feature, mean)))+
    geom_line(aes(group = feature), color = 'grey60')+
    geom_point(aes(color = value))+
    ylab("feature")+
    xlab("mean interaction rating")
  
}


baked |>
  select(game_id, name, yearpublished, interaction, starts_with("mechanics_")) |>
  pivot_categorical() |>
  mutate(feature = gsub("^mechanics_", "", feature)) |>
  boxplot_categorical()+
  theme_light()+
  labs(title = "Interactivity by Mechanic")

```

We notice something similar if we look at categories, with Fantasy and Fighting games tending to be more interactive than Economic games. Here we can also notice that certain categories that we would associate with party/social deduction games (Bluffing, Deduction) tend to have higher player interaction.

```{r}

baked |>
  select(game_id, name, yearpublished, interaction, starts_with("categories_")) |>
  pivot_categorical() |>
  mutate(feature = gsub("^categories_", "", feature)) |>
  boxplot_categorical()+
  theme_light()+
  labs(title = "Interactivity by Category")

```

I created a specific recipe just to deal with playercounts - I'm thinking that the best way to handle player counts might be to simply create dummies for individual player counts rather than just use the minimum and maximum player count.

```{r}

playercounts_recipe |>
  prep() |>
  bake(new_data = NULL) |>
  select(game_id, name, interaction, starts_with("player_counts")) |>
  pivot_longer(cols = starts_with("player_counts"),
               names_to = c("feature")) |>
  mutate(value = case_when(value > 0 ~ 'yes',
                           value < 0 ~ 'no')) |>
  plot_categorical(ncol = 2)

```


# Models

Now create model specifications for the purpose of tuning. I'll use models in the linear family for now, given the amount of data we have to train a model on. I'm just using a simple linear model, along with a lasso and a ridge regression to see how simple feature selection vs shrinkage helps.

```{r}

lm_mod = linear_reg()
ridge_mod = linear_reg(engine = "glmnet", 
                       penalty = tune::tune(),
                       mixture = 0)
pls_mod = pls(engine = "mixOmics",
              predictor_prop = tune::tune(),
              num_comp = tune::tune(),
              mode = "regression")
lasso_mod = linear_reg(engine = "glmnet", 
                       penalty = tune::tune(),
                       mixture = 1)

```

I'll then combine these models with the recipes to create a set of candidate workflows, which I will then tune and evaluate across resampling. I'm evaluating models using standard measures (rmse, rsquared, mape), but I'm also examining the  concordance correlation coefficient (ccc), as I think we really want to balance both accuracy and consistency.

```{r}

wflows = 
  workflowsets::workflow_set(
    preproc = list("baseline"  = baseline_recipe,
                   "expanded" = expanded_recipe,
                   "expanded+playercounts" = playercounts_recipe,
                   "full+playercounts" = full_recipe,
                   "pls" = pls_recipe,
                   "pca" = pca_recipe),
    models = list("lm" = lm_mod,
                  "lasso" = lasso_mod,
                  "ridge" = ridge_mod)) |>
  # remove combos not needed
  anti_join(
    expand_grid(
      model = c("baseline", "pca", "pls"),
      method = c("ridge", "lasso")
    ) |>
      unite("wflow_id", model, method, sep = "_"),
    by = "wflow_id"
  )
```

## Results

I'll now examine how each of these workflows performed during resampling.

```{r}
#| warning: false
#| message: false
#| cache: true
set.seed(1999)
models = 
  wflows |>
  workflow_map(fn = "tune_grid",
               resamples = folds,
               verbose = T,
               grid = 15,
               metrics = metric_set(rmse, rsq, mape, ccc),
               control = control_grid(save_pred = T, save_workflow = T)
  )

```

How did they all do? I'll just grab the resulting performance for every candidate model and plot it for each metric. 

```{r}
#| warning: false
#| message: false
models |>
  autoplot(type = "wflow_id")+
  theme_light()+
  theme(legend.position = 'top',
        legend.title = element_blank(),
        legend.text = element_text(size = 6))+
  scale_color_viridis_d(option = 'B')

```

Notice that there are more workflows shown in the plot than the ones that I fit - this is because for each of the glmnet models we are further evaluating a set of candidate models over its tuning parameter.

I'll just extract the best performing model for each workflow and see how it performs across these metrics. The best models, so far, are a simple linear model that uses partial least squares for feature extraction, followed by a ridge regression that uses all features + playercount dummies.

```{r}
#| class: scroll
models |> 
  rank_results(select_best = T, rank_metric = "ccc") |>
  select(wflow_id, .metric, mean) |>
  pivot_wider(names_from = c(".metric"),
              values_from = c("mean")) |>
  gt_tbl()|>
  gt::fmt_number(columns = c("ccc", "mape", "rmse", "rsq"), decimals = 3)
```

I'll extract predictions from the models and compare the predictions to the actual.

```{r}

collect_best = function(obj) {
  
  best_tuned = 
    obj |>
    mutate(best_tune = map(result, ~ show_best(.x, metric = 'ccc'))) |>
    select(wflow_id, best_tune) |>
    unnest(best_tune)
  
  # 
  preds =
    obj |>
    collect_predictions(summarize = T)
  
  preds |>
    inner_join(best_tuned)
  
}

plot_predictions = function(data) {
  
  data |>
    ggplot(aes(x=.pred, y=interaction, label = name))+
    geom_point(size = 1.5)+
    geom_text(size = 1.5, vjust = 1.5)+
    coord_cartesian(xlim = c(0.5, 5.5), ylim = c(0.5, 5.5))+
    geom_abline(slope = 1,
                linetype = 'dashed')+
    xlab("estimated interactivity")+
    ylab("actual interactivity")+
    ggpubr::stat_cor()
  
}

models |>
  collect_best() |>
  left_join(
    split |>
      training() |>
      mutate(.row = row_number()) |>
      select(game_id, name, .row)
  ) |>
  select(wflow_id, game_id, name, .row, .pred, interaction) |>
  ggplot(aes(x=.pred, y=interaction))+
  geom_point(size = 0.5)+
  facet_wrap(~wflow_id)+
  theme_light()+
  ggpubr::stat_cor()+
  geom_abline(slope = 1, linetype = 'dashed')

```

At this point, the best models look to be the penalized regressions and, interestingly, the linear model that used partial least squares as a dimension reduction method. I should explore that some more.

## Sparsity and Features

I'm curious to see sparse how partial least squares with a really rich set of features can do. Part of the problem with using categorical features in this setting is that many categories are quite sparse, which leads to dummy variables with mostly zeroes. For instance, while it might be useful to know that a game has a 'Prisoner's Dilemma' mechanic for the purpose of measuring interactivity, there might only be one game in the dataset that has this feature, which make it difficult for a model to find an effect for this feature.

Just looking at the games in this sample, we have over 100 different mechanics, the vast majority of which are only present in a handful of games. I'm generally not a fan of treemap charts, but I think it conveys the idea.

```{r}

rank_categorical = function(data, min = 1, breaks = 3) {
  
  data |>
    mutate(value = forcats::fct_lump_min(value, min = min)) |>
    group_by(type, value) |>
    count() |>
    ungroup() |>
    add_page_rank(breaks = breaks) |>
    mutate(page_rank = factor(page_rank))
  
}

add_page_rank = function(data, breaks = 5, ties = "min") {
  
  if (breaks > 1) {
    
    tmp = data |>
      mutate(rank = rank(-n, ties = ties),
             rank_cut = cut(rank, breaks = breaks, labels = F)) |>
      group_by(rank_cut) |>
      mutate(page_rank = paste(min(rank), max(rank), sep = "-")) |>
      ungroup() |>
      arrange(rank)
    
  } else if (breaks == 1) {
    tmp = data |>
      mutate(rank = rank(-n, ties = ties),
             page_rank =  paste(min(rank), max(rank), sep = "-")) |>
      arrange(rank)
  }
  
  tmp |>
    mutate(page_rank = factor(page_rank, levels = unique(tmp$page_rank)))
  
}

plot_ranks = function(data, ...) {
  
  data |> 
    ggplot(aes(y=reorder(value,n),
               x=n,
               text = paste(
                 paste(value),
                 paste("rank:", rank),
                 paste("games:", n),
                 sep = "\n")))+
    geom_col() +
    ylab("")+
    facet_wrap(page_rank~., scales = "free_y", ...)
}

plot_treemap = function(data, ...) {
  
  data |>
    treemap(
      index=c("type", "value"),
      vSize="n",
      vColor = "family_value",
      algorithm = "pivotSize",
      mirror.y = TRUE,
      mirror.x = TRUE,
      border.lwds = 0.7,
      aspRatio = 5/3,
      palette = "Paired",
      border.col = "white",
      ...
    )
}


games_raw |> 
  bggUtils:::unnest_mechanics() |>
  inner_join(
    games_and_interaction |>
      select(game_id, name, yearpublished, interaction),
    by = join_by(game_id)
  ) |>
  rank_categorical(min= 1, breaks = 2) |>
  plot_treemap(title = "Mechanics")

```

And there's also really interesting info to be gleaned from BGG families, such as components, mechanisms, countries, but there are so many of these that it's a challenge to even visualize. From the couple hundred games for which we have an interaction rating, the following plot displays all of the different types of BGG family variables that are present. 

```{r}




games_raw |> 
  bggUtils:::unnest_families() |>
  inner_join(
    games_and_interaction |>
      select(game_id, name, yearpublished, interaction),
    by = join_by(game_id)
  ) |>
  rename(family = type) |> 
  separate(value, into = c("type", "value"), sep = ": ") |>
  rank_categorical(min= 1, breaks = 2) |>
  plot_treemap(title = "BGG Families")


```

Zoom into just a few of these a bit more, we can probably spot some components and mechanisms that we would expect to be useful.

```{r}

games_raw |> 
  bggUtils:::unnest_families() |>
  inner_join(
    games_and_interaction |>
      select(game_id, name, yearpublished, interaction),
    by = join_by(game_id)
  ) |>
  rename(family = type) |> 
  separate(value, into = c("type", "value"), sep = ": ") |>
  filter(type %in% c("Mechanism", "Components", "Country", "Players")) |>
  rank_categorical(min= 1, breaks = 2) |>
  plot_treemap(title = "")

```

I'm going to try out a a recipe that allows for almost any of these as possible features into a model, and trust that a method like partial least squares can handle it. Basically this method first attempts to reduce the feature space by finding linear combinations of variables that maximize covariance with the outcome. It's similar-ish to PCA, but PCA but is purely unsupervised in aiming to find components that maximize variance. PLS is a supervised dimension reduction method, aiming to find latent variables that are associated with an outcome. 

Setting up the recipe and workflow.

```{r}

build_sparse_recipe = function(data, predictors, threshold = 1) {
  
  data |>
    build_recipe(outcome = interaction,
                 predictors = c("minplayers", 
                                "maxplayers",
                                "playingtime",
                                "maxplaytime",
                                "minplaytime",
                                "minage",
                                "mechanics",
                                "categories",
                                "families", 
                                "mechanisms",
                                "components",
                                "themes")
    ) |>
    step_playingtime_and_playercount() |>
    step_dummy_playercounts(threshold = threshold) |>
    step_count_items(mechanics, categories) |>
    add_dummies(mechanics, threshold = threshold) |>
    add_dummies(categories, threshold = threshold) |>
    add_dummies(families, threshold = threshold) |>
    add_dummies(mechanisms, threshold = threshold) |>
    add_dummies(components, threshold = threshold) |>
    add_dummies(themes, threshold = threshold) |>
    add_zv() |>
    add_normalize()
  
}

sparse_recipe = 
  split |>
  training() |>
  build_sparse_recipe()

```

I'll then tune this one over the number of components and proportion of predictors to include.

```{r}
#| cache: true
sparse_wflow = 
  workflow() |>
  add_model(
    pls_mod
  ) |>
  add_recipe(
    sparse_recipe
  )

sparse_tuned = 
  sparse_wflow |>
  tune_grid(
    grid = 15,
    resamples = folds,
    metrics = metric_set(rmse, rsq, mape, ccc),
    control = control_resamples(save_pred = T, save_workflow = T)
  )
```

Now extracting the results from tuning; looks like four components with a low proportion of predictors was our best result.

```{r}

sparse_tuned |>
  collect_metrics(type = 'wide') |>
  mutate_if(is.numeric, round, 3) |>
  arrange(desc(ccc)) |>
  gt_tbl()

```

How do the predictions from this method compare to what we found earlier?

```{r}

sparse_tuned |>
  collect_predictions(summarize = T) |>
  inner_join(
    sparse_tuned |>
      select_best(metric = 'ccc')
  ) |>
  arrange(.row) |>
  inner_join(
    games_and_interaction |>
      mutate(.row = row_number()) |>
      select(game_id, name, .row)
  ) |>
  plot_predictions()

```

## Inference

Okay so we've fit a decent number of models at this point, some of which look to be on the right track.

One of the best models at this stage is the ridge regression with player count dummies. I'll fit this model to the entirety of the training set to grab the coefficients and get a sense of what it's learning from the data. I'll also add a model postprocessor to constrain predictions between 1 and 5.

Looking at the coefficients, we can see (as before) that certain families (Two Player Only), mechanics (Take That, Area Majority), categories (Deduction, Territory Building), and player counts (5 players) are associated with higher interactivity ratings. Mechanics/mechanisms associated with Euro style games (End Game Bonuses, Tableau Building, Tile Placement) and low player counts (x2, x3) tend to reduce interactivity.

```{r}

# fit 
ridge_fit = 
  models |>
  filter(wflow_id == 'full+playercounts_ridge') |>
  fit_best(metric = "ccc")  |>
  add_tailor(
    tailor() |>
      adjust_numeric_range(lower_limit = 1, upper_limit = 5)
  )

# extract coefs
ridge_fit |>
  tidy() |>
  filter(term != "(Intercept)") |>
  slice_max(abs(estimate), n = 35) |>
  mutate(sign = case_when(estimate > 0 ~ 'increases interactivity',
                          estimate < 0 ~ 'decreases interactivity')) |>
  mutate(tidy_term = bggUtils::present_bgg_text(term)) |>
  ggplot(aes(y=reorder(tidy_term, estimate),
             x=estimate))+
  geom_col()+
  ylab("")+
  facet_wrap(~sign, scales = "free_x")+
  theme(axis.text.y = element_text(size = 6))

```

Now I'll take a look at the partial least squares model. It's a bit trickier to extract info from these, but basically we can get a sense of how individual features map to the latent variables (components), and we can also plot the components themselves compared to the outcome.

```{r}

sparse_fit =
  sparse_tuned |>
  fit_best(metric = "ccc")  |>
  add_tailor(
    tailor() |>
      adjust_numeric_range(lower_limit = 1, upper_limit = 5)
  )


```

First, look at how variables map to the loadings. I'll grab the top variables for each component - the scores themselves only have a directional interpretation, showing how different variables affect the 

```{r}

get_loadings = function(obj) {
  
  obj |>
    pluck("loadings", 1) |> 
    as.data.frame() |> 
    rownames_to_column("feature") |> 
    as_tibble()
  
}

sparse_fit |>
  extract_fit_engine() |> 
  get_loadings() |>
  pivot_longer(cols = starts_with("comp")) |>
  group_by(name) |>
  slice_max(n = 25, order_by = abs(value)) |>
  mutate(feature = bggUtils::present_bgg_text(feature),
         feature = gsub("Themes Theme", "Theme", feature)) |>
  ggplot(aes(x=value, y=tidytext::reorder_within(feature, abs(value), name)))+
  geom_col()+
  facet_wrap(name ~., scales = "free_y")+
  tidytext::scale_y_reordered()+
  ylab("")+
  theme(axis.text.y = element_text(size = 6),
        panel.grid.major = element_blank())

```

Next, the latent variables. I'll extract these and plot them against the interaction rating.

```{r}

sparse_fit |>
  extract_fit_engine() |>
  pluck("variates", 1) |>
  bind_cols(
    split |> 
      training() |>
      select(game_id, name, yearpublished, interaction)
  ) |>
  pivot_longer(
    cols = starts_with("comp"),
    names_to = c("component")
  ) |>
  ggplot(aes(x=interaction, y=value, label = name))+
  geom_point(alpha = 0.5, size = 1.5)+
  geom_text(size = 1.5, vjust = 1, check_overlap = T)+
  facet_wrap(component ~., scales = "free")

```

Definitely seeing some wonky stuff with games like Hive, Go, Patchwork; I'll need to dig into that some more. But that first component, man that looks almost exactly like ranking games from a continuum from 'multiplayer solitaire' to 'talking to people'. Really interesting.

## Validation

I'll take the ridge and partial least squares and evalaute their performance on the validation set. We'd like to see our estimated interactive rating be pretty close to the actual here; I want to see which approach does a better job on the validation set.

```{r}

valid_preds = 
  map(list("pls" = sparse_fit, "ridge" = ridge_fit),
      ~ .x |> 
        augment(split |> testing())
  ) |>
  bind_rows(.id = 'model') |>
  select(model, .pred, game_id, name, interaction)

valid_preds |> 
  ggplot(aes(x=.pred, y=interaction, label = name))+
  geom_point(size = 1.5)+
  geom_text(size = 1.5, vjust = 1.5)+
  coord_cartesian(xlim = c(0.5, 5.5), ylim = c(0.5, 5.5))+
  geom_abline(slope = 1,
              linetype = 'dashed')+
  xlab("estimated interactivity")+
  ylab("actual interactivity")+
  ggpubr::stat_cor()+
  facet_wrap(model ~.)

```

The biggest misses for both models look to be Railroad Ink and Troyes. I'd guess that the model thinks Railroad Ink has more interaction than it does because it has a low play time and can play at higher playercounts? Troyes I can see a model failing to pick up on - on paper it's a dry Euro game, but it's actually quite interactive because of you can block and buy each other's dice.

Where do they most disagree? I'll compare their predictions side by side. 

```{r}

valid_preds |>
  pivot_wider(names_from = c("model"), values_from = c(".pred")) |>
  ggplot(aes(x=pls, y=ridge, label = name))+
  geom_point(size = 2, alpha = 0.5)+
  geom_text(check_overlap = T, vjust = -1, size = 1.5)+
  geom_abline(slope = 1, linetype = 'dotted')

```

I wonder if an ensemble would be the next step? 

At any rate, each model's performance on the validation set is in line with what we found during resampling, which should alleviate fears of overfitting on such a small dataset.

```{r}

metrics = metric_set(rmse, rsq, mape, ccc)

valid_preds |>
  group_by(model) |>
  metrics(
    interaction,
    .pred
  ) |> 
  pivot_wider(names_from = c(".metric"), values_from = c(".estimate")) |>
  mutate_if(is.numeric, round, 3) |>
  gt_tbl()

```

Right now, the leader in the clubhouse is the partial least squares model, which is not something I was expecting going in. I'll fit that model to repeated resamples of the training set to get out of sample estimates for every game for which we do have a rating.

```{r}

best_mod = 
  models |>
  bind_rows(as_workflow_set(sparse_pls = sparse_tuned)) |>
  fit_best(metric = 'ccc') |>
  add_tailor(
    tailor() |>
      adjust_numeric_range(lower_limit = 1, upper_limit = 5)
  )


set.seed(1999)
folds_all = split$data |> vfold_cv(strata = interaction, repeats = 3)

oos_preds = 
  best_mod |>
  fit_resamples(resamples = folds_all,
                control = control_resamples(save_pred = T))

oos_estimates = 
  oos_preds |>
  collect_predictions(summarize = T) |>
  select(.row, .pred) |>
  left_join(
    split$data |>
      mutate(.row = row_number()) |>
      select(.row, game_id, name, yearpublished, interaction)
  )
```


```{r}

oos_estimates |>
  plot_predictions()

```


```{r}

estimates_tbl = function(data){
  
  data |>
    arrange(desc(interaction)) |>
    gt_tbl() |>
    gt::data_color(columns = c("estimate", "interaction"),
                   method = "numeric",
                   palette = "viridis",
                   domain = c(1, 5),
                   reverse = T) |>
    gt::cols_align(columns = c("estimate", "interaction"),
                   align = "center") |>
    gt::opt_interactive()
}


oos_estimates |>
  mutate_if(is.numeric, round, 3) |>
  select(game_id, name, estimate = .pred, interaction) |>
  arrange(desc(interaction)) |>
  mutate(game_id = as.character(game_id)) |>
  estimates_tbl() |>
  gt::cols_width(
    game_id ~ px(75)
  )

```


# Estimates

Now on to the task of estimating games for which we do not have ratings. I'll take the best performing model, fit it to the entirety of the data for which we have interaction ratings, and then predict games for which we do not.

```{r}

best_fit = 
  best_mod |>
  fit(
    split$data
  )

estimates = 
  best_fit |>
  predict(
    games_preprocessed 
  ) |>
  bind_cols(
    games_preprocessed
  )

```

The following plot shows the distribution of estimated interactivity for all games. Note that there's a bit of an uptick at 5 due to the fact that we're capping the rating at 5.

```{r}

estimates |>
  rename(interaction = .pred) |>
  ggplot(aes(x=interaction))+
  geom_histogram(bins = 80)

```

How does (estimated) interactivity compare to complexity? These two aren't expected to have a relationship, but I just want to see the "universe" of games based on these two dimensions. 

```{r}
#| message: false
#| warning: false
#| 
estimates |>
  select(game_id, name, averageweight, interaction = .pred) |>
  ggplot(aes(x=averageweight, y=interaction, label = name))+
  geom_point(size = 0.5, alpha = 0.25, position = ggforce::position_jitternormal())+
  geom_text(check_overlap = T, size = 1.5)+
  ylab("estimated interaction")

```

## Top 250

Let's filter to only games in the top 250 of BGG's geek rating, which will make this easier to see.  Let's look at them based on interaction vs complexity. How does this map to my expectations?

```{r}

df = 
  estimates |>
  rename(interaction = .pred) |>
  slice_max(bayesaverage, n = 250)

plot_interaction_vs_complexity = function(data) {
  
  data |>
    ggplot(aes(x=averageweight, y=interaction, label = name))+
    #geom_point(size = 2, alpha = 0.25, position = ggforce::position_jitternormal())+
    geom_text(check_overlap = T, size = 2)+
    ylab("estimated interaction")+
    geom_vline(xintercept = median(df$averageweight), linetype = 'dashed')+
    geom_hline(yintercept = median(df$interaction), linetype = 'dashed')+
    coord_cartesian(ylim = c(0.8, 5.2), xlim = c(0.8, 5.2))+
    annotate(geom = "label", y = 4.8, x=1.2, label = "low complexity \n high interaction", size = 2)+
    annotate(geom = "label", y = 1.2, x=4.6, label = "high complexity \n low interaction", size = 2)+
    annotate(geom = "label", y = 4.8, x=4.6, label = "high complexity \n high interaction", size = 2)+
    annotate(geom = "label", y = 1.2, x=1.2, label = "low complexity \n low interaction", size = 2)
}

df |>
  plot_interaction_vs_complexity()

```

Honestly this looks pretty good to my eye. 

```{r}

top_games = 
  estimates |>
  rename(interaction = .pred) |>
  slice_max(bayesaverage, n = 250) |>
  select(game_id, name, geek_rating = bayesaverage, averageweight, interaction) |>
  mutate_if(is.numeric, round, 3) |>
  arrange(interaction)

```

Which games in the top 250 are considered the least interactive? This looks to mostly be Euros and roll and writes that lean towards multiplayer solitaire.

```{r}
#| class: scroll
#| 
top_games |>
  arrange(interaction) |>
  head(25) |>
  gt_tbl() |>
  gt::data_color(columns = c("interaction"),
                 method = "numeric",
                 palette = "viridis",
                 domain = c(1, 5),
                 reverse = T) |>
  gt::cols_align(columns = c("interaction", "geek_rating"),
                 align = "center")

```

Which games are considered the most interactive? Wargames like Root, Rebellion, TI4, Twilight Struggle. Interactivity, in this case, is really picking up on the war/conflict/head to head side of things, though Five Tribes is one that's a lot higher up there than I would have expected?

```{r}

top_games |>
  arrange(desc(interaction)) |>
  head(25) |>
  gt_tbl() |>
  gt::data_color(columns = c("interaction"),
                 method = "numeric",
                 palette = "viridis",
                 domain = c(1, 5),
                 reverse = T) |>
  gt::cols_align(columns = c("interaction", "geek_rating"),
                 align = "center")

```

## Top 1000

Finally, here's the top 1000 games on BGG (just to make this table easier to) to sort through and examine along with other BGG ratings.

```{r}

estimates |>
  slice_max(bayesaverage, n = 1000) |>
  select(game_id, name, interaction = .pred, geek_rating = bayesaverage, averageweight, average) |>
  mutate_if(is.numeric, round, 3) |>
  gt_tbl() |>
  gt::opt_interactive() |>
  gt::data_color(columns = c("interaction"),
                 method = "numeric",
                 palette = "viridis",
                 domain = c(1, 5),
                 reverse = T) |>
  gt::cols_align(columns = c("interaction", "geek_rating", "averageweight"),
                 align = "center") |>
  gt::opt_interactive()
```

I'll also write out these estimates and save them to a CSV.

```{r}

estimates |>
  select(game_id, name, interaction = .pred) |>
  write.csv(file = "data/processed/estimates.csv")
```

